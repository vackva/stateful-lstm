{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:11:08.857713500Z",
     "start_time": "2024-01-17T16:11:08.831490100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from datetime import date, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose computation device (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:11:09.942199800Z",
     "start_time": "2024-01-17T16:11:09.910395200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the physical devices available:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "These are the visible devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f\"These are the physical devices available:\\n{physical_devices}\")\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    print(f\"These are the visible devices:\\n{visible_devices}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:38.940543Z",
     "start_time": "2024-01-17T17:58:38.732495900Z"
    }
   },
   "outputs": [],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "\n",
    "name = 'model_0'\n",
    "in_file = 'TrainingData/flanger-input.wav'\n",
    "out_file = 'TrainingData/flanger-target.wav'\n",
    "epochs = 1\n",
    "\n",
    "input_size = 1 \n",
    "batch_size = 4096\n",
    "test_size = 0.2\n",
    "learning_rate = 0.0005 \n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:40.420020600Z",
     "start_time": "2024-01-17T17:58:40.376533800Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    sp.io.wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:41.669523600Z",
     "start_time": "2024-01-17T17:58:41.468337300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape (pre-processing): (352800, 1)\n",
      "y_training shape (pre-processing): (352800, 1)\n",
      "X_testing shape (pre-processing): (88200, 1)\n",
      "y_testing shape (pre-processing): (88200, 1)\n",
      "X_ordered_training shape: (352256, 1, 1)\n",
      "X_ordered_testing shape: (86016, 1, 1)\n",
      "y_ordered_training shape: (352800, 1)\n",
      "y_ordered_testing shape: (88200, 1)\n",
      "X_random_training shape (post-processing): (352256, 1, 1)\n",
      "y_random_training shape (post-processing): (352256, 1)\n",
      "The X_random_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first 1 samples are grouped).\n",
      "The y_random_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data ###########################################\n",
    "in_rate, in_data = sp.io.wavfile.read(in_file)\n",
    "out_rate, out_data = sp.io.wavfile.read(out_file)\n",
    "\n",
    "X_all = in_data.astype(np.float32).flatten()  \n",
    "X_all = normalize(X_all).reshape(len(X_all),1)   \n",
    "y_all = out_data.astype(np.float32).flatten() \n",
    "y_all = normalize(y_all).reshape(len(y_all),1)\n",
    "\n",
    "# Get the last 20% of the wav data for testing and thee rest for training\n",
    "X_training, X_testing = np.split(X_all, [int(len(X_all)*(1-test_size))])\n",
    "y_training, y_testing = np.split(y_all, [int(len(y_all)*(1-test_size))])\n",
    "print(f\"X_training shape (pre-processing): {X_training.shape}\")\n",
    "print(f\"y_training shape (pre-processing): {y_training.shape}\")\n",
    "print(f\"X_testing shape (pre-processing): {X_testing.shape}\")\n",
    "print(f\"y_testing shape (pre-processing): {y_testing.shape}\")\n",
    "\n",
    "# Create a new array where each element is an array of input_size samples in time order\n",
    "# Each element of the new array is shifted by one sample from the previous element\n",
    "indices = np.arange(input_size) + np.arange(len(X_training)-input_size+1)[:,np.newaxis]\n",
    "X_ordered_training = tf.gather(X_training,indices) \n",
    "\n",
    "X_ordered_training_len = int(X_ordered_training.shape[0] / batch_size) * batch_size\n",
    "X_ordered_training = X_ordered_training[:X_ordered_training_len,:,:]\n",
    "\n",
    "print(f\"X_ordered_training shape: {X_ordered_training.shape}\")\n",
    "indices = np.arange(input_size) + np.arange(len(X_testing)-input_size+1)[:,np.newaxis]\n",
    "X_ordered_testing = tf.gather(X_testing,indices) \n",
    "\n",
    "\n",
    "X_ordered_testing_len = int(X_ordered_testing.shape[0] / batch_size) * batch_size\n",
    "X_ordered_testing = X_ordered_testing[:X_ordered_testing_len,:,:]\n",
    "\n",
    "print(f\"X_ordered_testing shape: {X_ordered_testing.shape}\")\n",
    "\n",
    "# The input size defines the number of samples used for each prediction\n",
    "# Therefore the first output value that we get is at index input_size-1\n",
    "y_ordered_training = y_training[input_size-1:]\n",
    "print(f\"y_ordered_training shape: {y_ordered_training.shape}\")\n",
    "y_ordered_testing = y_testing[input_size-1:]\n",
    "print(f\"y_ordered_testing shape: {y_ordered_testing.shape}\")\n",
    "\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(X_ordered_training)) \n",
    "X_random_training = tf.gather(X_ordered_training, shuffled_indices)\n",
    "y_random_training = tf.gather(y_ordered_training, shuffled_indices)\n",
    "\n",
    "X_random_training_len = int(X_random_training.shape[0] / batch_size) * batch_size\n",
    "y_random_training_len = int(y_random_training.shape[0] / batch_size) * batch_size\n",
    "\n",
    "X_random_training = X_random_training[:X_random_training_len,:,:]\n",
    "y_random_training = y_random_training[:y_random_training_len,:]\n",
    "\n",
    "print(f\"X_random_training shape (post-processing): {X_random_training.shape}\")\n",
    "print(f\"y_random_training shape (post-processing): {y_random_training.shape}\")\n",
    "\n",
    "print(f\"The X_random_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first {input_size} samples are grouped).\")\n",
    "print(f\"The y_random_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class StatefulLSTM(tf.keras.Model):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=1, batch_size=4096):\n",
    "        super(StatefulLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.skip = skip\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = keras.layers.LSTM(units=hidden_size, return_sequences=False, stateful=True, return_state=False, batch_size=batch_size, use_bias=True)    \n",
    "        self.dense = keras.layers.Dense(units=output_size, activation=None, batch_size=batch_size)\n",
    "        \n",
    "        # Build LSTM before training, because stateful lstm requires information batch size to build static graph\n",
    "        self.lstm.build((batch_size, input_size, 1))\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        x = self.lstm(input_tensor)\n",
    "        x = self.dense(x)    \n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:43.815804400Z",
     "start_time": "2024-01-17T17:58:43.778723100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "class ESRLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(ESRLoss, self).__init__()\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        energy = tf.reduce_mean(tf.square(y_true)) + self.epsilon\n",
    "        return loss / energy\n",
    "\n",
    "class DCLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(DCLoss, self).__init__()\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = tf.reduce_mean(tf.square(tf.reduce_mean(y_true, axis=0) - tf.reduce_mean(y_pred, axis=0)))\n",
    "        energy = tf.reduce_mean(tf.square(y_true)) + self.epsilon\n",
    "        return loss / energy\n",
    "\n",
    "class LossWrapper(tf.keras.losses.Loss):\n",
    "    def __init__(self, loss_weights):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        # Map the loss names to their corresponding classes\n",
    "        loss_dict = {'ESR': ESRLoss, 'DC': DCLoss}\n",
    "        # Create instances of the loss functions\n",
    "        self.loss_functions = [loss_dict[key]() for key in [\"ESR\", \"DC\"]]\n",
    "        # Assign the weights\n",
    "        self.loss_factors = [loss_weights[key] for key in [\"ESR\", \"DC\"]]\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        total_loss = 0\n",
    "        for i, loss_function in enumerate(self.loss_functions):\n",
    "            total_loss += loss_function(y_true, y_pred) * self.loss_factors[i]\n",
    "        return total_loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:47.962067800Z",
     "start_time": "2024-01-17T17:58:47.892861400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:50.916047600Z",
     "start_time": "2024-01-17T17:58:49.647496600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stateful_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 multiple                  4352      \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4385 (17.13 KB)\n",
      "Trainable params: 4385 (17.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "loss_wrapper = LossWrapper({\"ESR\": 0.75, \"DC\": 0.25})\n",
    "\n",
    "model = StatefulLSTM(input_size=1, \n",
    "                     output_size=1,\n",
    "                     hidden_size=32,\n",
    "                     skip=0,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "# Compile the model with the custom loss\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss=loss_wrapper)\n",
    "\n",
    "model.build((batch_size,1,1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T17:58:59.756272200Z",
     "start_time": "2024-01-17T17:58:55.674366200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 4s 15ms/step - loss: 0.6855\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_random_training, y=y_random_training, epochs=epochs, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "model.save_weights('models/' + name + '/weights')\n",
    "model.save_weights('models/' + name + '/lstm_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:02:47.251609300Z",
     "start_time": "2024-01-17T18:02:47.114421800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "### 0. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "inference_batch_size = 2\n",
    "\n",
    "inference_model = StatefulLSTM(input_size=1, \n",
    "                               output_size=1,\n",
    "                               hidden_size=32,\n",
    "                               skip=0,\n",
    "                               batch_size=inference_batch_size)\n",
    "\n",
    "input_shape = (inference_batch_size,1,1)\n",
    "\n",
    "inference_model.build(input_shape)\n",
    "\n",
    "inference_model.load_weights('models/' + name + '/lstm_model.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:18:13.695691600Z",
     "start_time": "2024-01-17T18:18:13.445911400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. On the test audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T15:54:59.420106500Z",
     "start_time": "2024-01-17T15:54:58.914932500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "42/42 [==============================] - 0s 7ms/step\n",
      "X_testing shape:  (88200, 1)\n",
      "X_ordered_testing shape:  (86016, 1, 1)\n",
      "y_testing shape:  (88200, 1)\n",
      "prediction shape:  (86016, 1)\n",
      "Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Run Prediction #################################################\n",
    "# Test the model on the testing data #############################\n",
    "print(\"Running prediction..\")\n",
    "\n",
    "input_data = X_ordered_testing\n",
    "\n",
    "# Ensure the input data size for inference is a multiple of the batch size used during training.\n",
    "# The batch size is crucial as the stateful LSTM maintains internal states for a specific number of samples across batches.\n",
    "# Use model.predict(input_data, batch_size) to process the entire dataset in the specified batch size.\n",
    "# Alternatively, use predict_on_batch for single batch processing, but this requires that the input_data len equals the specified batch size  \n",
    "# Note: Mismatch in batch size or non-multiples of batch size in input data may lead to errors or unexpected model behavior.\n",
    "#model.reset_states(2048)\n",
    "prediction = inference_model.predict(input_data, inference_batch_size)\n",
    "\n",
    "save_wav('models/'+name+'/y_pred.wav', prediction)\n",
    "save_wav('models/'+name+'/x_test.wav', X_testing)\n",
    "save_wav('models/'+name+'/y_test.wav', y_testing)\n",
    "\n",
    "print(\"X_testing shape: \", X_testing.shape)\n",
    "print(\"X_ordered_testing shape: \", X_ordered_testing.shape)\n",
    "print(\"y_testing shape: \", y_testing.shape)\n",
    "print(\"prediction shape: \", prediction.shape)\n",
    "\n",
    "print(\"Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. On a number sequence (to control inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T18:18:24.990115300Z",
     "start_time": "2024-01-17T18:18:24.443152600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "1/1 [==============================] - 0s 390ms/step\n",
      "prediction [[-0.00425536]\n",
      " [-0.00425536]]\n",
      "Running prediction..\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "prediction2 [[-0.00707254]\n",
      " [-0.00707254]]\n",
      "X_testing_2 shape:  (2, 1, 1)\n",
      "prediction_2 shape:  (2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the model simple number sequence to compare with inference \n",
    "input_shape = (2,1,1)\n",
    "test_sequence = tf.zeros(input_shape)\n",
    "\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction = inference_model.predict(test_sequence)\n",
    "print(f\"prediction {prediction}\")\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction = inference_model.predict(test_sequence)\n",
    "print(f\"prediction2 {prediction}\")\n",
    "\n",
    "print(\"X_testing_2 shape: \", test_sequence.shape)\n",
    "print(\"prediction_2 shape: \", prediction.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as tflite model\n",
    "### 1. for minimal examples (with batch size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:07:38.906872100Z",
     "start_time": "2024-01-17T16:07:30.958686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stateful_lstm_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               multiple                  4352      \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4385 (17.13 KB)\n",
      "Trainable params: 4385 (17.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 377ms/step\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\VALENT~1\\AppData\\Local\\Temp\\tmp4cb_oo6w\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\VALENT~1\\AppData\\Local\\Temp\\tmp4cb_oo6w\\assets\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "C:\\Users\\Valentin Ackva\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65:0: error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nC:\\Users\\Valentin Ackva\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConverterError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Convert to TensorFlow Lite format\u001B[39;00m\n\u001B[0;32m     19\u001B[0m converter \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mlite\u001B[38;5;241m.\u001B[39mTFLiteConverter\u001B[38;5;241m.\u001B[39mfrom_keras_model(save_model)\n\u001B[1;32m---> 20\u001B[0m tflite_model \u001B[38;5;241m=\u001B[39m \u001B[43mconverter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Save the TensorFlow Lite model\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels/\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39mname\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39mname\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-minimal.tflite\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1065\u001B[0m, in \u001B[0;36m_export_metrics.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1062\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(convert_func)\n\u001B[0;32m   1063\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1064\u001B[0m   \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m-> 1065\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_and_export_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1042\u001B[0m, in \u001B[0;36mTFLiteConverterBase._convert_and_export_metrics\u001B[1;34m(self, convert_func, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1040\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_conversion_params_metric()\n\u001B[0;32m   1041\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mprocess_time()\n\u001B[1;32m-> 1042\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1043\u001B[0m elapsed_time_ms \u001B[38;5;241m=\u001B[39m (time\u001B[38;5;241m.\u001B[39mprocess_time() \u001B[38;5;241m-\u001B[39m start_time) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m\n\u001B[0;32m   1044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1526\u001B[0m, in \u001B[0;36mTFLiteKerasModelConverterV2.convert\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1513\u001B[0m \u001B[38;5;129m@_export_metrics\u001B[39m\n\u001B[0;32m   1514\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Converts a keras model based on instance variables.\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \n\u001B[0;32m   1517\u001B[0m \u001B[38;5;124;03m  Returns:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;124;03m      Invalid quantization parameters.\u001B[39;00m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1526\u001B[0m   saved_model_convert_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_as_saved_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1527\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m saved_model_convert_result:\n\u001B[0;32m   1528\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saved_model_convert_result\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1507\u001B[0m, in \u001B[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1503\u001B[0m   graph_def, input_tensors, output_tensors \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1504\u001B[0m       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_keras_to_saved_model(temp_dir)\n\u001B[0;32m   1505\u001B[0m   )\n\u001B[0;32m   1506\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msaved_model_dir:\n\u001B[1;32m-> 1507\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mTFLiteKerasModelConverterV2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph_def\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_tensors\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1511\u001B[0m   shutil\u001B[38;5;241m.\u001B[39mrmtree(temp_dir, \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1296\u001B[0m, in \u001B[0;36mTFLiteConverterBaseV2.convert\u001B[1;34m(self, graph_def, input_tensors, output_tensors)\u001B[0m\n\u001B[0;32m   1289\u001B[0m   logging\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   1290\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing new converter: If you encounter a problem \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1291\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease file a bug. You can opt-out \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1292\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby setting experimental_new_converter=False\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1293\u001B[0m   )\n\u001B[0;32m   1295\u001B[0m \u001B[38;5;66;03m# Converts model.\u001B[39;00m\n\u001B[1;32m-> 1296\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43m_convert_graphdef\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1297\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_def\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1298\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1299\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1300\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconverter_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1301\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1303\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimize_tflite_model(\n\u001B[0;32m   1304\u001B[0m     result, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_quant_mode, quant_io\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_new_quantizer\n\u001B[0;32m   1305\u001B[0m )\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:212\u001B[0m, in \u001B[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     report_error_message(\u001B[38;5;28mstr\u001B[39m(converter_error))\n\u001B[1;32m--> 212\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m converter_error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Re-throws the exception.\u001B[39;00m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m    214\u001B[0m   report_error_message(\u001B[38;5;28mstr\u001B[39m(error))\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001B[0m, in \u001B[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    204\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m ConverterError \u001B[38;5;28;01mas\u001B[39;00m converter_error:\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m converter_error\u001B[38;5;241m.\u001B[39merrors:\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:918\u001B[0m, in \u001B[0;36mconvert_graphdef\u001B[1;34m(input_data, input_tensors, output_tensors, **kwargs)\u001B[0m\n\u001B[0;32m    915\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    916\u001B[0m     model_flags\u001B[38;5;241m.\u001B[39moutput_arrays\u001B[38;5;241m.\u001B[39mappend(util\u001B[38;5;241m.\u001B[39mget_tensor_name(output_tensor))\n\u001B[1;32m--> 918\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_flags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconversion_flags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    921\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSerializeToString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdebug_info_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug_info\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSerializeToString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdebug_info\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_mlir_converter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_mlir_converter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py:367\u001B[0m, in \u001B[0;36mconvert\u001B[1;34m(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)\u001B[0m\n\u001B[0;32m    359\u001B[0m         conversion_flags\u001B[38;5;241m.\u001B[39mguarantee_all_funcs_one_use \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    360\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m convert(\n\u001B[0;32m    361\u001B[0m             model_flags,\n\u001B[0;32m    362\u001B[0m             conversion_flags,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    365\u001B[0m             enable_mlir_converter,\n\u001B[0;32m    366\u001B[0m         )\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converter_error\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _run_deprecated_conversion_binary(\n\u001B[0;32m    370\u001B[0m     model_flags\u001B[38;5;241m.\u001B[39mSerializeToString(),\n\u001B[0;32m    371\u001B[0m     conversion_flags\u001B[38;5;241m.\u001B[39mSerializeToString(),\n\u001B[0;32m    372\u001B[0m     input_data_str,\n\u001B[0;32m    373\u001B[0m     debug_info_str,\n\u001B[0;32m    374\u001B[0m )\n",
      "\u001B[1;31mConverterError\u001B[0m: C:\\Users\\Valentin Ackva\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65:0: error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\nC:\\Users\\Valentin Ackva\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall\"]): called from\n<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\\n converter._experimental_lower_tensor_list_ops = False\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the desired input shape\n",
    "save_model.build((1,1,1))  # Ensure this shape is correct for your model\n",
    "\n",
    "# Load the pretrained weights\n",
    "save_model.load_weights('lstm_model.h5')\n",
    "\n",
    "test_data = tf.zeros((1,1,1))\n",
    "prediction = save_model.predict(test_data, 1)\n",
    "\n",
    "# Convert to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(save_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open(\"models/\"+name+\"/\"+name+\"-minimal.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# Uncomment to analyze the model\n",
    "# tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\VALENT~1\\AppData\\Local\\Temp\\tmptyp_zqu0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\VALENT~1\\AppData\\Local\\Temp\\tmptyp_zqu0\\assets\n"
     ]
    }
   ],
   "source": [
    "input_shape = [1, 1, 1]\n",
    "\n",
    "func = tf.function(inference_model).get_concrete_function(\n",
    "    tf.TensorSpec(input_shape, dtype=tf.float32))\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([func], inference_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(\"models/\"+name+\"/\"+\"steerable-nafx.tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:03:08.598685800Z",
     "start_time": "2024-01-17T18:03:00.124542900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\VALENT~1\\AppData\\Local\\Temp\\tmpt5hgngpr\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\VALENT~1\\AppData\\Local\\Temp\\tmpt5hgngpr\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite ModelAnalyzer ===\n",
      "\n",
      "Your TFLite model has '4' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the CALL_ONCE op takes\n",
      " as input and produces  as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#23]\n",
      "  Op#0 CALL_ONCE(Subgraph#1) -> []\n",
      "  Op#1 VAR_HANDLE() -> [T#11]\n",
      "  Op#2 VAR_HANDLE() -> [T#12]\n",
      "  Op#3 READ_VARIABLE(T#11) -> [T#13]\n",
      "  Op#4 READ_VARIABLE(T#12) -> [T#14]\n",
      "  Op#5 TRANSPOSE(T#0, T#8[1, 0, 2]) -> [T#15]\n",
      "  Op#6 WHILE(T#9[0], T#9[0], T#4, T#14, T#13, T#15, Cond: Subgraph#2, Body: Subgraph#3) -> [T#16, T#17, T#18, T#19, T#20, T#21]\n",
      "  Op#7 STRIDED_SLICE(T#18, T#1[-1, 0, 0], T#2[0, 4096, 32], T#3[1, 1, 1]) -> [T#22]\n",
      "  Op#8 FULLY_CONNECTED(T#22, T#10, T#5) -> [T#23]\n",
      "  Op#9 ASSIGN_VARIABLE(T#12, T#19) -> []\n",
      "  Op#10 ASSIGN_VARIABLE(T#11, T#20) -> []\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(serving_default_input_1:0) shape_signature:[-1, 1, 1], type:FLOAT32\n",
      "  T#1(strided_slice_2) shape:[3], type:INT32 RO 12 bytes, buffer: 2, data:[-1, 0, 0]\n",
      "  T#2(strided_slice_21) shape:[3], type:INT32 RO 12 bytes, buffer: 3, data:[0, 4096, 32]\n",
      "  T#3(strided_slice_22) shape:[3], type:INT32 RO 12 bytes, buffer: 4, data:[1, 1, 1]\n",
      "  T#4(TensorArrayV2_1) shape:[1, 4096, 32], type:FLOAT32 RO 524288 bytes, buffer: 5, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#5(stateful_lstm/dense/BiasAdd/ReadVariableOp) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 6, data:[0.000770824]\n",
      "  T#6(stateful_lstm/lstm/Read_4/ReadVariableOp) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 7, data:[0.00112093, 0.0522929, 0.0459044, 0.0115302, -0.0341365, ...]\n",
      "  T#7(TensorArrayV2_1/num_elements) shape:[], type:INT32 RO 4 bytes, buffer: 8, data:[1]\n",
      "  T#8(transpose/perm) shape:[3], type:INT32 RO 12 bytes, buffer: 9, data:[1, 0, 2]\n",
      "  T#9(time) shape:[], type:INT32 RO 4 bytes, buffer: 10, data:[0]\n",
      "  T#10(stateful_lstm/dense/MatMul) shape:[1, 32], type:FLOAT32 RO 128 bytes, buffer: 11, data:[0.0225239, -0.0607256, -0.221962, 0.0266721, 0.158595, ...]\n",
      "  T#11(Variable) shape:[], type:RESOURCE\n",
      "  T#12(Variable_1) shape:[], type:RESOURCE\n",
      "  T#13(stateful_lstm/lstm/Read_1/ReadVariableOp) shape:[4096, 32], type:FLOAT32\n",
      "  T#14(stateful_lstm/lstm/Read/ReadVariableOp) shape:[4096, 32], type:FLOAT32\n",
      "  T#15(transpose) shape_signature:[1, -1, 1], type:FLOAT32\n",
      "  T#16(while) shape:[], type:INT32\n",
      "  T#17(while1) shape:[], type:INT32\n",
      "  T#18(while2) shape:[1, 4096, 32], type:FLOAT32\n",
      "  T#19(while3) shape:[4096, 32], type:FLOAT32\n",
      "  T#20(while4) shape:[4096, 32], type:FLOAT32\n",
      "  T#21(while5) shape_signature:[1, -1, 1], type:FLOAT32\n",
      "  T#22(strided_slice_23) shape:[4096, 32], type:FLOAT32\n",
      "  T#23(StatefulPartitionedCall:0) shape:[4096, 1], type:FLOAT32\n",
      "\n",
      "Subgraph#1 NoOp() -> []\n",
      "  Op#0 VAR_HANDLE() -> [T#1_2]\n",
      "  Op#1 ASSIGN_VARIABLE(T#1_2, T#1_1) -> []\n",
      "  Op#2 VAR_HANDLE() -> [T#1_3]\n",
      "  Op#3 ASSIGN_VARIABLE(T#1_3, T#1_0) -> []\n",
      "\n",
      "Tensors of Subgraph#1\n",
      "  T#1_0(arith.constant) shape:[4096, 32], type:FLOAT32 RO 524288 bytes, buffer: 25, data:[-0.0122549, -0.039028, 0.00329951, 0.0392959, 0.0847436, ...]\n",
      "  T#1_1(arith.constant1) shape:[4096, 32], type:FLOAT32 RO 524288 bytes, buffer: 26, data:[-0.00618164, -0.0201755, 0.00171874, 0.0200549, 0.0407464, ...]\n",
      "  T#1_2(Variable_11) shape:[], type:RESOURCE\n",
      "  T#1_3(Variable1) shape:[], type:RESOURCE\n",
      "\n",
      "Subgraph#2 while_cond(T#2_0, T#2_1, T#2_2, T#2_3, T#2_4, T#2_5) -> [T#2_7]\n",
      "  Op#0 LESS(T#2_1, T#2_6[1]) -> [T#2_7]\n",
      "\n",
      "Tensors of Subgraph#2\n",
      "  T#2_0(arg0) shape:[], type:INT32\n",
      "  T#2_1(arg1) shape:[], type:INT32\n",
      "  T#2_2(arg2) shape:[1, 4096, 32], type:FLOAT32\n",
      "  T#2_3(arg3) shape:[4096, 32], type:FLOAT32\n",
      "  T#2_4(arg4) shape:[4096, 32], type:FLOAT32\n",
      "  T#2_5(arg5) shape_signature:[1, -1, 1], type:FLOAT32\n",
      "  T#2_6(TensorArrayV2_1/num_elements1) shape:[], type:INT32 RO 4 bytes, buffer: 35, data:[1]\n",
      "  T#2_7(while/Less) shape:[], type:BOOL\n",
      "\n",
      "Subgraph#3 while_body(T#3_0, T#3_1, T#3_2, T#3_3, T#3_4, T#3_5) -> [T#3_31, T#3_11, T#3_30, T#3_29, T#3_27, T#3_5]\n",
      "  Op#0 ADD(T#3_1, T#3_7[1]) -> [T#3_11]\n",
      "  Op#1 FULLY_CONNECTED(T#3_3, T#3_10, T#-1) -> [T#3_12]\n",
      "  Op#2 GATHER(T#3_5, T#3_1) -> [T#3_13]\n",
      "  Op#3 FULLY_CONNECTED(T#3_13, T#3_9, T#-1) -> [T#3_14]\n",
      "  Op#4 ADD(T#3_14, T#3_12) -> [T#3_15]\n",
      "  Op#5 ADD(T#3_15, T#3_6) -> [T#3_16]\n",
      "  Op#6 SPLIT(T#3_7[1], T#3_16) -> [T#3_17, T#3_18, T#3_19, T#3_20]\n",
      "  Op#7 LOGISTIC(T#3_17) -> [T#3_21]\n",
      "  Op#8 LOGISTIC(T#3_18) -> [T#3_22]\n",
      "  Op#9 MUL(T#3_22, T#3_4) -> [T#3_23]\n",
      "  Op#10 LOGISTIC(T#3_20) -> [T#3_24]\n",
      "  Op#11 TANH(T#3_19) -> [T#3_25]\n",
      "  Op#12 MUL(T#3_21, T#3_25) -> [T#3_26]\n",
      "  Op#13 ADD(T#3_23, T#3_26) -> [T#3_27]\n",
      "  Op#14 TANH(T#3_27) -> [T#3_28]\n",
      "  Op#15 MUL(T#3_24, T#3_28) -> [T#3_29]\n",
      "  Op#16 RESHAPE(T#3_29, T#3_8[1, 4096, 32]) -> [T#3_30]\n",
      "  Op#17 ADD(T#3_0, T#3_7[1]) -> [T#3_31]\n",
      "\n",
      "Tensors of Subgraph#3\n",
      "  T#3_0(arg0) shape:[], type:INT32\n",
      "  T#3_1(arg1) shape:[], type:INT32\n",
      "  T#3_2(arg2) shape:[1, 4096, 32], type:FLOAT32\n",
      "  T#3_3(arg3) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_4(arg4) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_5(arg5) shape_signature:[1, -1, 1], type:FLOAT32\n",
      "  T#3_6(stateful_lstm/lstm/Read_4/ReadVariableOp1) shape:[128], type:FLOAT32 RO 512 bytes, buffer: 43, data:[0.00112093, 0.0522929, 0.0459044, 0.0115302, -0.0341365, ...]\n",
      "  T#3_7(TensorArrayV2_1/num_elements2) shape:[], type:INT32 RO 4 bytes, buffer: 35, data:[1]\n",
      "  T#3_8(while/TensorArrayV2Write/TensorListSetItem) shape:[3], type:INT32 RO 12 bytes, buffer: 45, data:[1, 4096, 32]\n",
      "  T#3_9(while/MatMul) shape:[128, 1], type:FLOAT32 RO 512 bytes, buffer: 46, data:[-0.00357177, -0.216209, 0.0162345, 0.03668, -0.190488, ...]\n",
      "  T#3_10(while/MatMul_11) shape:[128, 32], type:FLOAT32 RO 16384 bytes, buffer: 47, data:[-0.201755, 0.0533911, 0.0462771, 0.087205, 0.0681612, ...]\n",
      "  T#3_11(while/add_2) shape:[], type:INT32\n",
      "  T#3_12(while/MatMul_12) shape:[4096, 128], type:FLOAT32\n",
      "  T#3_13(while/TensorArrayV2Read/TensorListGetItem;time) shape_signature:[-1, 1], type:FLOAT32\n",
      "  T#3_14(while/MatMul1) shape_signature:[-1, 128], type:FLOAT32\n",
      "  T#3_15(while/add) shape:[4096, 128], type:FLOAT32\n",
      "  T#3_16(while/BiasAdd) shape:[4096, 128], type:FLOAT32\n",
      "  T#3_17(while/split) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_18(while/split1) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_19(while/split2) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_20(while/split3) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_21(while/Sigmoid) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_22(while/Sigmoid_1) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_23(while/mul) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_24(while/Sigmoid_2) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_25(while/Tanh) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_26(while/mul_1) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_27(while/add_1) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_28(while/Tanh_1) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_29(while/mul_2) shape:[4096, 32], type:FLOAT32\n",
      "  T#3_30(while/TensorArrayV2Write/TensorListSetItem1) shape:[1, 4096, 32], type:FLOAT32\n",
      "  T#3_31(while/add_3) shape:[], type:INT32\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Your TFLite model has '1' signature_def(s).\n",
      "\n",
      "Signature#0 key: 'serving_default'\n",
      "- Subgraph: Subgraph#0\n",
      "- Inputs: \n",
      "    'input_1' : T#0\n",
      "- Outputs: \n",
      "    'output_1' : T#23\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:    1599120 bytes\n",
      "    Non-data buffer size:       8032 bytes (00.50 %)\n",
      "  Total data buffer size:    1591088 bytes (99.50 %)\n",
      "          - Subgraph#0  :     524988 bytes (32.83 %)\n",
      "          - Subgraph#1  :    1048576 bytes (65.57 %)\n",
      "          - Subgraph#2  :          4 bytes (00.00 %)\n",
      "          - Subgraph#3  :      17424 bytes (01.09 %)\n",
      "    (Zero value buffers):     524292 bytes (32.79 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  (Consider use `converter._experimental_unfold_large_splat_constant` to save the model size.)\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n"
     ]
    }
   ],
   "source": [
    "# Convert to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open(\"models/\"+name+\"/\"+name+\"-minimal.tflite\", 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "#tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T17:59:17.990986100Z",
     "start_time": "2024-01-17T17:59:09.541680300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T16:05:02.415370Z",
     "start_time": "2024-01-17T16:05:01.908780Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf2onnx.tf_loader:TF freezing failed. Attempting to fix freezing errors.\n",
      "WARNING:tf2onnx.tf_loader:Removed Sigmoid stateful_lstm/lstm/AssignVariableOp_1\n",
      "WARNING:tf2onnx.tf_loader:Removed Sigmoid stateful_lstm/lstm/AssignVariableOp\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "# Define the input shape\n",
    "input_signature = [tf.TensorSpec([4096, input_size, 1], tf.float32, name='x')]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(proto=onnx_model, f=\"models/\"+name+\"/\"+name+\"-tflite\"+\"-minimal.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. for real-time streaming (with batch size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_streaming = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Please consider providing the trackable_obj argument in the from_concrete_functions. Providing without the trackable_obj argument is deprecated and it will use the deprecated conversion path.\n",
      "2023-10-24 15:22:31.259267: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 15:22:31.259339: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 15:22:31.260768: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2023-10-24 15:22:31.260791: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-10-24 15:22:31.260795: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-10-24 15:22:31.260854: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:22:31.260891: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 15:22:31.395033: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-10-24 15:22:31.395046: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TFLite ModelAnalyzer ===\n",
      "\n",
      "Your TFLite model has '3' subgraph(s). In the subgraph description below,\n",
      "T# represents the Tensor numbers. For example, in Subgraph#0, the PAD op takes\n",
      "tensor #0 and tensor #15 as input and produces tensor #21 as output.\n",
      "\n",
      "Subgraph#0 main(T#0) -> [T#37]\n",
      "  Op#0 PAD(T#0, T#15[0, 0, 12, 12, 0, ...]) -> [T#21]\n",
      "  Op#1 RESHAPE(T#21, T#3[128, 1, 174, 1]) -> [T#22]\n",
      "  Op#2 CONV_2D(T#22, T#7, T#1) -> [T#23]\n",
      "  Op#3 RESHAPE(T#23, T#4[128, 14, 16]) -> [T#24]\n",
      "  Op#4 PAD(T#24, T#15[0, 0, 12, 12, 0, ...]) -> [T#25]\n",
      "  Op#5 RESHAPE(T#25, T#5[128, 1, 38, 16]) -> [T#26]\n",
      "  Op#6 CONV_2D(T#26, T#8, T#2) -> [T#27]\n",
      "  Op#7 RESHAPE(T#27, T#6[128, 3, 16]) -> [T#28]\n",
      "  Op#8 TRANSPOSE(T#28, T#17[1, 0, 2]) -> [T#29]\n",
      "  Op#9 WHILE(T#19[0], T#19[0], T#12, T#13, T#13, T#29, Cond: Subgraph#1, Body: Subgraph#2) -> [T#30, T#31, T#32, T#33, T#34, T#35]\n",
      "  Op#10 STRIDED_SLICE(T#32, T#9[-1, 0, 0], T#10[0, 128, 36], T#11[1, 1, 1]) -> [T#36]\n",
      "  Op#11 FULLY_CONNECTED(T#36, T#20, T#18) -> [T#37]\n",
      "\n",
      "Tensors of Subgraph#0\n",
      "  T#0(args_0) shape:[128, 150, 1], type:FLOAT32\n",
      "  T#1(model/conv1d/BiasAdd/ReadVariableOp/resource) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 2, data:[-0.00639128, -0.00703551, 0.00228243, -0.0137976, -0.00964908, ...]\n",
      "  T#2(model/conv1d_1/BiasAdd/ReadVariableOp/resource) shape:[16], type:FLOAT32 RO 64 bytes, buffer: 3, data:[0.0183422, 0.0140636, -0.0220616, -0.0199623, -0.0959258, ...]\n",
      "  T#3(model/conv1d/Conv1D/ExpandDims) shape:[4], type:INT32 RO 16 bytes, buffer: 4, data:[128, 1, 174, 1]\n",
      "  T#4(model/conv1d/Conv1D/Squeeze) shape:[3], type:INT32 RO 12 bytes, buffer: 5, data:[128, 14, 16]\n",
      "  T#5(model/conv1d_1/Conv1D/ExpandDims) shape:[4], type:INT32 RO 16 bytes, buffer: 6, data:[128, 1, 38, 16]\n",
      "  T#6(model/conv1d_1/Conv1D/Squeeze) shape:[3], type:INT32 RO 12 bytes, buffer: 7, data:[128, 3, 16]\n",
      "  T#7(model/conv1d/Conv1D) shape:[16, 1, 12, 1], type:FLOAT32 RO 768 bytes, buffer: 8, data:[0.379673, 0.24539, 0.492858, 0.612726, 0.503913, ...]\n",
      "  T#8(model/conv1d_1/Conv1D) shape:[16, 1, 12, 16], type:FLOAT32 RO 12288 bytes, buffer: 9, data:[0.242897, -0.571169, -0.61761, -0.190251, -0.234319, ...]\n",
      "  T#9(strided_slice_2;model/lstm/PartitionedCall/strided_slice_2) shape:[3], type:INT32 RO 12 bytes, buffer: 10, data:[-1, 0, 0]\n",
      "  T#10(strided_slice_2;model/lstm/PartitionedCall/strided_slice_21) shape:[3], type:INT32 RO 12 bytes, buffer: 11, data:[0, 128, 36]\n",
      "  T#11(strided_slice_2;model/lstm/PartitionedCall/strided_slice_22) shape:[3], type:INT32 RO 12 bytes, buffer: 12, data:[1, 1, 1]\n",
      "  T#12(TensorArrayV2_1;model/lstm/PartitionedCall/TensorArrayV2_1) shape:[1, 128, 36], type:FLOAT32 RO 18432 bytes, buffer: 13, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#13(model/lstm/zeros) shape:[128, 36], type:FLOAT32 RO 18432 bytes, buffer: 13, data:[0, 0, 0, 0, 0, ...]\n",
      "  T#14(strided_slice;model/lstm/PartitionedCall/strided_slice) shape:[], type:INT32 RO 4 bytes, buffer: 15, data:[3]\n",
      "  T#15(model/zero_padding1d/Pad/paddings) shape:[3, 2], type:INT32 RO 24 bytes, buffer: 16, data:[0, 0, 12, 12, 0, ...]\n",
      "  T#16(model/lstm/Read_2/ReadVariableOp/resource) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 17, data:[-0.273044, -0.291953, 0.675976, 0.218638, -0.132412, ...]\n",
      "  T#17(transpose/perm;model/lstm/PartitionedCall/transpose/perm) shape:[3], type:INT32 RO 12 bytes, buffer: 18, data:[1, 0, 2]\n",
      "  T#18(model/dense/BiasAdd/ReadVariableOp/resource) shape:[1], type:FLOAT32 RO 4 bytes, buffer: 19, data:[-0.00218528]\n",
      "  T#19(model/conv1d/Conv1D/ExpandDims_1/dim) shape:[], type:INT32 RO 4 bytes, buffer: 20, data:[0]\n",
      "  T#20(model/dense/MatMul) shape:[1, 36], type:FLOAT32 RO 144 bytes, buffer: 21, data:[0.0928863, 0.212152, 0.142203, -0.0439729, 0.0866588, ...]\n",
      "  T#21(model/zero_padding1d/Pad) shape:[128, 174, 1], type:FLOAT32\n",
      "  T#22(model/conv1d/Conv1D/ExpandDims1) shape:[128, 1, 174, 1], type:FLOAT32\n",
      "  T#23(model/conv1d/BiasAdd;model/conv1d/Conv1D/Squeeze;model/conv1d/BiasAdd/ReadVariableOp/resource;model/conv1d_1/Conv1D;model/conv1d/Conv1D) shape:[128, 1, 14, 16], type:FLOAT32\n",
      "  T#24(model/conv1d/BiasAdd;model/conv1d/Conv1D/Squeeze;model/conv1d/BiasAdd/ReadVariableOp/resource) shape:[128, 14, 16], type:FLOAT32\n",
      "  T#25(model/zero_padding1d_1/Pad) shape:[128, 38, 16], type:FLOAT32\n",
      "  T#26(model/conv1d_1/Conv1D/ExpandDims1) shape:[128, 1, 38, 16], type:FLOAT32\n",
      "  T#27(model/conv1d_1/BiasAdd;model/conv1d_1/Conv1D/Squeeze;model/conv1d_1/BiasAdd/ReadVariableOp/resource;model/conv1d_1/Conv1D) shape:[128, 1, 3, 16], type:FLOAT32\n",
      "  T#28(model/conv1d_1/BiasAdd;model/conv1d_1/Conv1D/Squeeze;model/conv1d_1/BiasAdd/ReadVariableOp/resource) shape:[128, 3, 16], type:FLOAT32\n",
      "  T#29(transpose;model/lstm/PartitionedCall/transpose) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#30(while;model/lstm/PartitionedCall/while) shape:[], type:INT32\n",
      "  T#31(while;model/lstm/PartitionedCall/while1) shape:[], type:INT32\n",
      "  T#32(while;model/lstm/PartitionedCall/while2) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#33(while;model/lstm/PartitionedCall/while3) shape:[128, 36], type:FLOAT32\n",
      "  T#34(while;model/lstm/PartitionedCall/while4) shape:[128, 36], type:FLOAT32\n",
      "  T#35(while;model/lstm/PartitionedCall/while5) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#36(strided_slice_2;model/lstm/PartitionedCall/strided_slice_23) shape:[128, 36], type:FLOAT32\n",
      "  T#37(Identity) shape:[128, 1], type:FLOAT32\n",
      "\n",
      "Subgraph#1 while;model/lstm/PartitionedCall/while_cond(T#1_0, T#1_1, T#1_2, T#1_3, T#1_4, T#1_5) -> [T#1_7]\n",
      "  Op#0 LESS(T#1_1, T#1_6[3]) -> [T#1_7]\n",
      "\n",
      "Tensors of Subgraph#1\n",
      "  T#1_0(arg0) shape:[], type:INT32\n",
      "  T#1_1(arg1) shape:[], type:INT32\n",
      "  T#1_2(arg2) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#1_3(arg3) shape:[128, 36], type:FLOAT32\n",
      "  T#1_4(arg4) shape:[128, 36], type:FLOAT32\n",
      "  T#1_5(arg5) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#1_6(strided_slice;model/lstm/PartitionedCall/strided_slice1) shape:[], type:INT32 RO 4 bytes, buffer: 45, data:[3]\n",
      "  T#1_7(while/Less) shape:[], type:BOOL\n",
      "\n",
      "Subgraph#2 while;model/lstm/PartitionedCall/while_body(T#2_0, T#2_1, T#2_2, T#2_3, T#2_4, T#2_5) -> [T#2_31, T#2_11, T#2_30, T#2_29, T#2_27, T#2_5]\n",
      "  Op#0 ADD(T#2_1, T#2_10[1]) -> [T#2_11]\n",
      "  Op#1 FULLY_CONNECTED(T#2_3, T#2_9, T#-1) -> [T#2_12]\n",
      "  Op#2 GATHER(T#2_5, T#2_1) -> [T#2_13]\n",
      "  Op#3 FULLY_CONNECTED(T#2_13, T#2_8, T#-1) -> [T#2_14]\n",
      "  Op#4 ADD(T#2_14, T#2_12) -> [T#2_15]\n",
      "  Op#5 ADD(T#2_15, T#2_6) -> [T#2_16]\n",
      "  Op#6 SPLIT(T#2_10[1], T#2_16) -> [T#2_17, T#2_18, T#2_19, T#2_20]\n",
      "  Op#7 LOGISTIC(T#2_17) -> [T#2_21]\n",
      "  Op#8 LOGISTIC(T#2_18) -> [T#2_22]\n",
      "  Op#9 MUL(T#2_22, T#2_4) -> [T#2_23]\n",
      "  Op#10 LOGISTIC(T#2_20) -> [T#2_24]\n",
      "  Op#11 TANH(T#2_19) -> [T#2_25]\n",
      "  Op#12 MUL(T#2_21, T#2_25) -> [T#2_26]\n",
      "  Op#13 ADD(T#2_23, T#2_26) -> [T#2_27]\n",
      "  Op#14 TANH(T#2_27) -> [T#2_28]\n",
      "  Op#15 MUL(T#2_24, T#2_28) -> [T#2_29]\n",
      "  Op#16 RESHAPE(T#2_29, T#2_7[1, 128, 36]) -> [T#2_30]\n",
      "  Op#17 ADD(T#2_0, T#2_10[1]) -> [T#2_31]\n",
      "\n",
      "Tensors of Subgraph#2\n",
      "  T#2_0(arg0) shape:[], type:INT32\n",
      "  T#2_1(arg1) shape:[], type:INT32\n",
      "  T#2_2(arg2) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#2_3(arg3) shape:[128, 36], type:FLOAT32\n",
      "  T#2_4(arg4) shape:[128, 36], type:FLOAT32\n",
      "  T#2_5(arg5) shape:[3, 128, 16], type:FLOAT32\n",
      "  T#2_6(model/lstm/Read_2/ReadVariableOp/resource1) shape:[144], type:FLOAT32 RO 576 bytes, buffer: 53, data:[-0.273044, -0.291953, 0.675976, 0.218638, -0.132412, ...]\n",
      "  T#2_7(while/TensorArrayV2Write/TensorListSetItem) shape:[3], type:INT32 RO 12 bytes, buffer: 54, data:[1, 128, 36]\n",
      "  T#2_8(while/MatMul) shape:[144, 16], type:FLOAT32 RO 9216 bytes, buffer: 55, data:[-0.00901565, -0.131223, -0.0329599, -0.137585, -0.0901903, ...]\n",
      "  T#2_9(while/MatMul_11) shape:[144, 36], type:FLOAT32 RO 20736 bytes, buffer: 56, data:[-0.262119, -0.323021, 0.0694119, -0.0371232, 0.696546, ...]\n",
      "  T#2_10(while/add_2/y) shape:[], type:INT32 RO 4 bytes, buffer: 57, data:[1]\n",
      "  T#2_11(while/add_2) shape:[], type:INT32\n",
      "  T#2_12(while/MatMul_12) shape:[128, 144], type:FLOAT32\n",
      "  T#2_13(while/TensorArrayV2Read/TensorListGetItem;model/conv1d/Conv1D/ExpandDims_1/dim) shape:[128, 16], type:FLOAT32\n",
      "  T#2_14(while/MatMul1) shape:[128, 144], type:FLOAT32\n",
      "  T#2_15(while/add) shape:[128, 144], type:FLOAT32\n",
      "  T#2_16(while/BiasAdd) shape:[128, 144], type:FLOAT32\n",
      "  T#2_17(while/split) shape:[128, 36], type:FLOAT32\n",
      "  T#2_18(while/split1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_19(while/split2) shape:[128, 36], type:FLOAT32\n",
      "  T#2_20(while/split3) shape:[128, 36], type:FLOAT32\n",
      "  T#2_21(while/Sigmoid) shape:[128, 36], type:FLOAT32\n",
      "  T#2_22(while/Sigmoid_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_23(while/mul) shape:[128, 36], type:FLOAT32\n",
      "  T#2_24(while/Sigmoid_2) shape:[128, 36], type:FLOAT32\n",
      "  T#2_25(while/Tanh) shape:[128, 36], type:FLOAT32\n",
      "  T#2_26(while/mul_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_27(while/add_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_28(while/Tanh_1) shape:[128, 36], type:FLOAT32\n",
      "  T#2_29(while/mul_2) shape:[128, 36], type:FLOAT32\n",
      "  T#2_30(while/TensorArrayV2Write/TensorListSetItem1) shape:[1, 128, 36], type:FLOAT32\n",
      "  T#2_31(while/add_3) shape:[], type:INT32\n",
      "\n",
      "---------------------------------------------------------------\n",
      "              Model size:      72688 bytes\n",
      "    Non-data buffer size:       9564 bytes (13.16 %)\n",
      "  Total data buffer size:      63124 bytes (86.84 %)\n",
      "          - Subgraph#0  :      50908 bytes (70.04 %)\n",
      "          - Subgraph#1  :          4 bytes (00.01 %)\n",
      "          - Subgraph#2  :      30544 bytes (42.02 %)\n",
      "    (Zero value buffers):      18436 bytes (25.36 %)\n",
      "\n",
      "* Buffers of TFLite model are mostly used for constant tensors.\n",
      "  And zero value buffers are buffers filled with zeros.\n",
      "  (Consider use `converter._experimental_unfold_large_splat_constant` to save the model size.)\n",
      "  Non-data buffers area are used to store operators, subgraphs and etc.\n",
      "  You can find more details from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/schema/schema.fbs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 15:22:31.440740: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2245] Estimated count of arithmetic ops: 6.538 M  ops, equivalently 3.269 M  MACs\n"
     ]
    }
   ],
   "source": [
    "input_shape = model.inputs[0].shape.as_list()\n",
    "input_shape[0] = batch_size_streaming\n",
    "func = tf.function(model).get_concrete_function(\n",
    "    tf.TensorSpec(input_shape, model.inputs[0].dtype))\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([func])\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(\"models/\"+name+\"/\"+name+\"-streaming.tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "tf.lite.experimental.Analyzer.analyze(model_content=tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf2onnx.tf_loader:Could not search for non-variable resources. Concrete function internal representation may have changed.\n",
      "2023-10-24 15:22:35.191437: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 15:22:35.191501: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 15:22:35.191641: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:22:35.191653: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 15:22:35.402321: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2023-10-24 15:22:35.402382: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2023-10-24 15:22:35.402491: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:22:35.402500: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-24 15:22:35.464633: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "# Define the input shape\n",
    "input_signature = [tf.TensorSpec([batch_size_streaming, input_size, 1], tf.float32, name='x')]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)\n",
    "onnx.save(proto=onnx_model, f=\"models/\"+name+\"/\"+name+\"-tflite\"+\"-streaming.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping layer: <keras.src.engine.input_layer.InputLayer object at 0x10469b0d0>\n"
     ]
    }
   ],
   "source": [
    "# Save the model as a JSON file (from RTNeural repo) ###################################\n",
    "import model_utils_RTNeural\n",
    "\n",
    "model_utils_RTNeural.save_model(model, filename=\"models/\"+name+\"/\"+name+\".json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
