{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/69/65/0d47953afa0ad569d12de5f65d964321c208492064c38fe3b0b9744f8d44/numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/2a/27/cd2e60d4accf81aa6279be8c5e9bca99a16bd21b3c0428dc515569e36561/torch-2.1.2-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached torch-2.1.2-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/54/4b/b0861005f5d4370b3529f31d5e6461c4faf9bbbcbe916480cceebc885aa8/torchvision-0.16.2-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached torchvision-0.16.2-cp38-cp38-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio\n",
      "  Obtaining dependency information for torchaudio from https://files.pythonhosted.org/packages/2a/36/96895107b44f41cda87bf41d344cc7b7a1a4be8fff9bda1c66da5ab30051/torchaudio-2.1.2-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached torchaudio-2.1.2-cp38-cp38-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/70/25/fab23259a52ece5670dcb8452e1af34b89e6135ecc17cd4b54b4b479eac6/fsspec-2023.12.2-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://files.pythonhosted.org/packages/0d/a9/1f4c54afaf2e689ba40b7688095bb70be8e84ec206c7dfe156c0645a0d52/pillow-10.2.0-cp38-cp38-win_amd64.whl.metadata\n",
      "  Using cached pillow-10.2.0-cp38-cp38-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Using cached torch-2.1.2-cp38-cp38-win_amd64.whl (192.3 MB)\n",
      "Using cached torchvision-0.16.2-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Using cached torchaudio-2.1.2-cp38-cp38-win_amd64.whl (2.3 MB)\n",
      "Using cached pillow-10.2.0-cp38-cp38-win_amd64.whl (2.6 MB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Installing collected packages: mpmath, sympy, pillow, numpy, networkx, fsspec, filelock, torch, scipy, torchvision, torchaudio\n",
      "Successfully installed filelock-3.13.1 fsspec-2023.12.2 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 pillow-10.2.0 scipy-1.10.1 sympy-1.12 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scipy torch torchvision torchaudio"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-03T11:59:31.144478700Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:41:50.904470600Z",
     "start_time": "2024-01-04T15:41:48.630418500Z"
    }
   },
   "id": "d80fc465400cc842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "This code is ...."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f57b51c451debb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Code\n",
    "\n",
    "Define all necessary functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "358fae992edd"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def prepareTrainingData(config):\n",
    "    in_rate, in_data = wavfile.read(config[\"input_audio_path\"])\n",
    "    out_rate, out_data = wavfile.read(config[\"target_audio_path\"])\n",
    "    \n",
    "    if len(in_data) != len(out_data):\n",
    "        print(\"input and target files have different lengths\")\n",
    "        sys.exit()\n",
    "      \n",
    "    if len(in_data.shape) > 1 or len(out_data.shape) > 1:\n",
    "        print(\"expected mono files\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Convert PCM16 to FP32\n",
    "    if in_data.dtype == \"int16\":\n",
    "        in_data = in_data / 32767\n",
    "        print(\"In data converted from PCM16 to FP32\")\n",
    "    if out_data.dtype == \"int16\":\n",
    "        out_data = out_data / 32767\n",
    "        print(\"Out data converted from PCM16 to FP32\")    \n",
    "\n",
    "    clean_data = in_data.astype(np.float32).flatten()\n",
    "    target_data = out_data.astype(np.float32).flatten()\n",
    "\n",
    "    # Split the data on a twenty percent mod\n",
    "    in_train, out_train, in_val, out_val = sliceOnMod(clean_data, target_data)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-input.wav\", in_train)\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-target.wav\", out_train)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-target.wav\", out_val)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-target.wav\", out_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:41:52.625336400Z",
     "start_time": "2024-01-04T15:41:52.593553200Z"
    }
   },
   "id": "1cdc8701863f502b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sliceOnMod(input_data, target_data, mod=5):\n",
    "    # Split the data on a modulus.\n",
    "\n",
    "    # Type cast to an integer the modulus\n",
    "    mod = int(mod)\n",
    "\n",
    "    # Split the data into 100 pieces\n",
    "    input_split = np.array_split(input_data, 100)\n",
    "    target_split = np.array_split(target_data, 100)\n",
    "\n",
    "    val_input_data = []\n",
    "    val_target_data = []\n",
    "    # Traverse the range of the indexes of the input signal reversed and pop every 5th for val\n",
    "    for i in reversed(range(len(input_split))):\n",
    "        if i % mod == 0:\n",
    "            # Store the validation data\n",
    "            val_input_data.append(input_split[i])\n",
    "            val_target_data.append(target_split[i])\n",
    "            # Remove the validation data from training\n",
    "            input_split.pop(i)\n",
    "            target_split.pop(i)\n",
    "\n",
    "    # Flatten val_data down to one dimension and concatenate\n",
    "    val_input_data = np.concatenate(val_input_data)\n",
    "    val_target_data = np.concatenate(val_target_data)\n",
    "\n",
    "    # Concatenate back together\n",
    "    training_input_data = np.concatenate(input_split)\n",
    "    training_target_data = np.concatenate(target_split)\n",
    "    return (training_input_data, training_target_data, val_input_data, val_target_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:41:54.013629700Z",
     "start_time": "2024-01-04T15:41:53.998058300Z"
    }
   },
   "id": "d2db02127a930a4c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    directory = os.path.dirname(name)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:41:55.547344300Z",
     "start_time": "2024-01-04T15:41:55.518442400Z"
    }
   },
   "id": "8ac5a30c5ce341b1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In data converted from PCM16 to FP32\n",
      "Out data converted from PCM16 to FP32\n"
     ]
    }
   ],
   "source": [
    "importConfig = {\n",
    "    \"input_audio_path\": \"TrainingData/flanger-input.wav\",\n",
    "    \"target_audio_path\": \"TrainingData/flanger-target.wav\",\n",
    "    \"output_path\": \"Data\",\n",
    "    \"name\": \"flanger\"\n",
    "}\n",
    "\n",
    "prepareTrainingData(importConfig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:41:56.487422400Z",
     "start_time": "2024-01-04T15:41:56.455832300Z"
    }
   },
   "id": "c7cc0b6f364b5692"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f2ed6f6e582365"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=1, bias_fl=True, num_layers=1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        # Create dictionary of possible block types\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.lin = nn.Linear(hidden_size, output_size, bias=bias_fl)\n",
    "        self.bias_fl = bias_fl\n",
    "        self.skip = skip\n",
    "        self.save_state = True\n",
    "        self.hidden = (torch.zeros(1, 1, 32), \n",
    "                       torch.zeros(1, 1, 32))\n",
    "\n",
    "    # Origin forward function \n",
    "    def forward(self, x):    \n",
    "        if self.skip:\n",
    "            # save the residual for the skip connection\n",
    "            res = x[:, :, 0:self.skip]\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x) + res\n",
    "        else:\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x)\n",
    "\n",
    "    # detach hidden state, this resets gradient tracking on the hidden state\n",
    "    def detach_hidden(self):\n",
    "        if self.hidden.__class__ == tuple:\n",
    "            self.hidden = tuple([h.clone().detach() for h in self.hidden])\n",
    "        else:\n",
    "            self.hidden = self.hidden.clone().detach()\n",
    "\n",
    "    # changes the hidden state to None, causing pytorch to create an all-zero hidden state when the rec unit is called\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.hidden = (torch.zeros(1, batch_size, 32), \n",
    "                       torch.zeros(1, batch_size, 32))\n",
    "\n",
    "    # This functions saves the model and all its paraemters to a json file, so it can be loaded by a JUCE plugin\n",
    "    def save_model(self, file_name, direc=''):\n",
    "        model_data = {'model_data': {'model': 'SimpleRNN', 'input_size': self.lstm.input_size, 'skip': self.skip,\n",
    "                                     'output_size': self.lin.out_features, 'unit_type': self.lstm._get_name(),\n",
    "                                     'num_layers': self.lstm.num_layers, 'hidden_size': self.lstm.hidden_size,\n",
    "                                     'bias_fl': self.bias_fl}}\n",
    "\n",
    "        if self.save_state:\n",
    "            model_state = self.state_dict()\n",
    "            for each in model_state:\n",
    "                model_state[each] = model_state[each].tolist()\n",
    "            model_data['state_dict'] = model_state\n",
    "\n",
    "        json_save(model_data, file_name, direc)\n",
    "\n",
    "        # Scripting the model for compatibility with LibTorch\n",
    "        \n",
    "        self.reset_hidden(1)\n",
    "        \n",
    "        model_scripted = torch.jit.script(self)\n",
    "        # \n",
    "        # # Saving the scripted model\n",
    "        scripted_model_file = file_name + \"_scripted.pt\"\n",
    "        if direc:\n",
    "             scripted_model_file = os.path.join(direc, scripted_model_file)\n",
    "        model_scripted.save(scripted_model_file)\n",
    "\n",
    "    # train_epoch runs one epoch of training\n",
    "    def train_epoch(self, input_data, target_data, loss_fcn, optim, bs, init_len=200, up_fr=1000):\n",
    "        # shuffle the segments at the start of the epoch\n",
    "        shuffle = torch.randperm(input_data.shape[1])\n",
    "\n",
    "        self.reset_hidden(bs)\n",
    "        \n",
    "        # Iterate over the batches\n",
    "        ep_loss = 0\n",
    "        for batch_i in range(math.ceil(shuffle.shape[0] / bs)):\n",
    "            # Load batch of shuffled segments\n",
    "            input_batch = input_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "            target_batch = target_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "\n",
    "            # Initialise network hidden state by processing some samples then zero the gradient buffers\n",
    "            self(input_batch[0:init_len, :, :])\n",
    "            self.zero_grad()\n",
    "\n",
    "            # Choose the starting index for processing the rest of the batch sequence, in chunks of args.up_fr\n",
    "            start_i = init_len\n",
    "            batch_loss = 0\n",
    "            # Iterate over the remaining samples in the mini batch\n",
    "            for k in range(math.ceil((input_batch.shape[0] - init_len) / up_fr)):\n",
    "                # Process input batch with neural network\n",
    "                output = self(input_batch[start_i:start_i + up_fr, :, :])\n",
    "\n",
    "                # Calculate loss and update network parameters\n",
    "                loss = loss_fcn(output, target_batch[start_i:start_i + up_fr, :, :])\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Set the network hidden state, to detach it from the computation graph\n",
    "                self.detach_hidden()\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Update the start index for the next iteration and add the loss to the batch_loss total\n",
    "                start_i += up_fr\n",
    "                batch_loss += loss\n",
    "\n",
    "            # Add the average batch loss to the epoch loss and reset the hidden states to zeros\n",
    "            ep_loss += batch_loss / (k + 1)\n",
    "        return ep_loss / (batch_i + 1)\n",
    "\n",
    "    # only proc processes a the input data and calculates the loss, optionally grad can be tracked or not\n",
    "    def process_data(self, input_data, target_data, loss_fcn, chunk, grad=False):\n",
    "        with (torch.no_grad() if not grad else nullcontext()):\n",
    "            self.reset_hidden(input_data.shape[1])\n",
    "            output = torch.empty_like(target_data)\n",
    "            for l in range(int(output.size()[0] / chunk)):\n",
    "                output[l * chunk:(l + 1) * chunk] = self(input_data[l * chunk:(l + 1) * chunk])\n",
    "                self.detach_hidden()\n",
    "            # If the data set doesn't divide evenly into the chunk length, process the remainder\n",
    "            if not (output.size()[0] / chunk).is_integer():\n",
    "                output[(l + 1) * chunk:-1] = self(input_data[(l + 1) * chunk:-1])\n",
    "            loss = loss_fcn(output, target_data)\n",
    "        return output, loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:24:50.438950600Z",
     "start_time": "2024-01-04T18:24:50.422951800Z"
    }
   },
   "id": "860763dc7078f73d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ESRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESRLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.add(target, -output)\n",
    "        loss = torch.pow(loss, 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss\n",
    "class DCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.pow(torch.add(torch.mean(target, 0), -torch.mean(output, 0)), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss      \n",
    "class LossWrapper(nn.Module):\n",
    "    def __init__(self, losses):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        loss_dict = {'ESR': ESRLoss(), 'DC': DCLoss()}\n",
    "\n",
    "        loss_functions = [[loss_dict[key], value] for key, value in losses.items()]\n",
    "\n",
    "        self.loss_functions = tuple([items[0] for items in loss_functions])\n",
    "        try:\n",
    "            self.loss_factors = tuple(torch.Tensor([items[1] for items in loss_functions]))\n",
    "        except IndexError:\n",
    "            self.loss_factors = torch.ones(len(self.loss_functions))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = 0\n",
    "        for i, losses in enumerate(self.loss_functions):\n",
    "            loss += torch.mul(losses(output, target), self.loss_factors[i])\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:42:00.521108400Z",
     "start_time": "2024-01-04T15:42:00.495456100Z"
    }
   },
   "id": "cbd60e7e671135d4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Splits audio, each split marker determines the fraction of the total audio in that split, i.e [0.75, 0.25] will put\n",
    "# 75% in the first split and 25% in the second\n",
    "def audio_splitter(audio, split_markers):\n",
    "    assert sum(split_markers) <= 1.0\n",
    "    if sum(split_markers) < 0.999:\n",
    "        print(\"sum of split markers is less than 1, so not all audio will be included in dataset\")\n",
    "    start = 0\n",
    "    slices = []\n",
    "    # convert split markers to samples\n",
    "    split_bounds = [int(x * audio.shape[0]) for x in split_markers]\n",
    "    for n in split_bounds:\n",
    "        end = start + n\n",
    "        slices.append(audio[start:end])\n",
    "        start = end\n",
    "    return slices\n",
    "\n",
    "# converts numpy audio into frames, and creates a torch tensor from them, frame_len = 0 just converts to a torch tensor\n",
    "def framify(audio, frame_len):\n",
    "    # If audio is mono, add a dummy dimension, so the same operations can be applied to mono/multichannel audio\n",
    "    audio = np.expand_dims(audio, 1) if len(audio.shape) == 1 else audio\n",
    "    # Calculate the number of segments the training data will be split into in frame_len is not 0\n",
    "    seg_num = math.floor(audio.shape[0] / frame_len) if frame_len else 1\n",
    "    # If no frame_len is provided, set frame_len to be equal to length of the input audio\n",
    "    frame_len = audio.shape[0] if not frame_len else frame_len\n",
    "    # Find the number of channels\n",
    "    channels = audio.shape[1]\n",
    "    # Initialise tensor matrices\n",
    "    dataset = torch.empty((frame_len, seg_num, channels))\n",
    "    # Load the audio for the training set\n",
    "    for i in range(seg_num):\n",
    "        dataset[:, i, :] = torch.from_numpy(audio[i * frame_len:(i + 1) * frame_len, :])\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:42:03.084013800Z",
     "start_time": "2024-01-04T15:42:03.075480100Z"
    }
   },
   "id": "9eda68cb093d2ea1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data_dir='../Dataset/', extensions=('input', 'target')):\n",
    "        self.extensions = extensions if extensions else ['']\n",
    "        self.subsets = {}\n",
    "        assert type(data_dir) == str, \"data_dir should be string,not %r\" % {type(data_dir)}\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    # add a subset called 'name', desired 'frame_len' is given in seconds, or 0 for just one long frame\n",
    "    def create_subset(self, name, frame_len=0):\n",
    "        assert type(name) == str, \"data subset name must be a string, not %r\" %{type(name)}\n",
    "        assert not (name in self.subsets), \"subset %r already exists\" %name\n",
    "        self.subsets[name] = SubSet(frame_len)\n",
    "\n",
    "    # load a file of 'filename' into existing subset/s 'set_names', split fractionally as specified by 'splits',\n",
    "    # if 'cond_val' is provided the conditioning value will be saved along with the frames of the loaded data\n",
    "    def load_file(self, filename, set_names='train', splits=None, cond_val=None):\n",
    "        # Assertions and checks\n",
    "        if type(set_names) == str:\n",
    "            set_names = [set_names]\n",
    "        assert len(set_names) == 1 or len(set_names) == len(splits), \"number of subset names must equal number of \" \\\n",
    "                                                                     \"split markers\"\n",
    "        assert [self.subsets.get(each) for each in set_names], \"set_names contains subsets that don't exist yet\"\n",
    "\n",
    "        # Load each of the 'extensions'\n",
    "        for i, ext in enumerate(self.extensions):\n",
    "            try:\n",
    "                file_loc = os.path.join(self.data_dir, filename + '-' + ext)\n",
    "                file_loc = file_loc + '.wav' if not file_loc.endswith('.wav') else file_loc\n",
    "                np_data = wavfile.read(file_loc)\n",
    "            except FileNotFoundError:\n",
    "                file_loc = os.path.join(self.data_dir, filename + ext)\n",
    "                file_loc = file_loc + '.wav' if not file_loc.endswith('.wav') else file_loc\n",
    "                np_data = wavfile.read(file_loc)\n",
    "            except FileNotFoundError:\n",
    "                print([\"File Not Found At: \" + self.data_dir + filename])\n",
    "                return\n",
    "            #print(\"np_data\", np_data[1])  # KAB DEBUG ######################################\n",
    "            #print(\"np_data dtype\", np_data[1].dtype)  # KAB DEBUG ######################################\n",
    "            raw_audio = np_data[1].astype(np.float32)\n",
    "            #raw_audio = np_data[1]  # KAB DEBUG ######################################\n",
    "            #print(\"raw_audio\", raw_audio)  # KAB DEBUG ######################################\n",
    "            # Split the audio if the set_names were provided\n",
    "            if len(set_names) > 1:\n",
    "                raw_audio = audio_splitter(raw_audio, splits)\n",
    "                for n, sets in enumerate(set_names):\n",
    "                    self.subsets[set_names[n]].add_data(np_data[0], raw_audio[n], ext, cond_val)\n",
    "            elif len(set_names) == 1:\n",
    "                self.subsets[set_names[0]].add_data(np_data[0], raw_audio, ext, cond_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:42:04.324019800Z",
     "start_time": "2024-01-04T15:42:04.312507700Z"
    }
   },
   "id": "8ada66f7a022959"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# The SubSet class holds a subset of data,\n",
    "# frame_len sets the length of audio per frame (in s), if set to 0 a single frame is used instead\n",
    "class SubSet:\n",
    "    def __init__(self, frame_len):\n",
    "        self.data = {}\n",
    "        self.cond_data = {}\n",
    "        self.frame_len = frame_len\n",
    "        self.conditioning = None\n",
    "        self.fs = None\n",
    "\n",
    "    # Add 'audio' data, in the data dictionary at the key 'ext', if cond_val is provided save the cond_val of each frame\n",
    "    def add_data(self, fs, audio, ext, cond_val):\n",
    "        if not self.fs:\n",
    "            self.fs = fs\n",
    "        assert self.fs == fs, \"data with different sample rate provided to subset\"\n",
    "        # if no 'ext' is provided, all the subsets data will be stored at the 'data' key of the 'data' dict\n",
    "        ext = 'data' if not ext else ext\n",
    "        # Frame the data and optionally create a tensor of the conditioning values of each frame\n",
    "        framed_data = framify(audio, self.frame_len)\n",
    "        cond_data = cond_val * torch.ones(framed_data.shape[1]) if isinstance(cond_val, (float, int)) else None\n",
    "\n",
    "        try:\n",
    "            # Convert data from tuple to list and concatenate new data onto the data tensor\n",
    "            data = list(self.data[ext])\n",
    "            self.data[ext] = (torch.cat((data[0], framed_data), 1),)\n",
    "            # If cond_val is provided add it to the cond_val tensor, note all frames or no frames must have cond vals\n",
    "            if isinstance(cond_val, (float, int)):\n",
    "                assert torch.is_tensor(self.cond_data[ext][0]), 'cond val provided, but previous data has no cond val'\n",
    "                c_data = list(self.cond_data[ext])\n",
    "                self.cond_data[ext] = (torch.cat((c_data[0], cond_data), 0),)\n",
    "        # If this is the first data to be loaded into the subset, create the data and cond_data tuples\n",
    "        except KeyError:\n",
    "            self.data[ext] = (framed_data,)\n",
    "            self.cond_data[ext] = (cond_data,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:42:06.251661400Z",
     "start_time": "2024-01-04T15:42:06.236604300Z"
    }
   },
   "id": "cb8fc7bf9b21f1f5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class TrainTrack(dict):\n",
    "    def __init__(self):\n",
    "        self.update({'current_epoch': 0, 'training_losses': [], 'validation_losses': [], 'train_av_time': 0.0,\n",
    "                     'val_av_time': 0.0, 'total_time': 0.0, 'best_val_loss': 1e12, 'test_loss': 0})\n",
    "\n",
    "    def restore_data(self, training_info):\n",
    "        self.update(training_info)\n",
    "\n",
    "    def train_epoch_update(self, loss, ep_st_time, ep_end_time, init_time, current_ep):\n",
    "        if self['train_av_time']:\n",
    "            self['train_av_time'] = (self['train_av_time'] + ep_end_time - ep_st_time) / 2\n",
    "        else:\n",
    "            self['train_av_time'] = ep_end_time - ep_st_time\n",
    "        self['training_losses'].append(loss)\n",
    "        self['current_epoch'] = current_ep\n",
    "        self['total_time'] += ((init_time + ep_end_time - ep_st_time)/3600)\n",
    "\n",
    "    def val_epoch_update(self, loss, ep_st_time, ep_end_time):\n",
    "        if self['val_av_time']:\n",
    "            self['val_av_time'] = (self['val_av_time'] + ep_end_time - ep_st_time) / 2\n",
    "        else:\n",
    "            self['val_av_time'] = ep_end_time - ep_st_time\n",
    "        self['validation_losses'].append(loss)\n",
    "        if loss < self['best_val_loss']:\n",
    "            self['best_val_loss'] = loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:42:07.785229900Z",
     "start_time": "2024-01-04T15:42:07.747136Z"
    }
   },
   "id": "706337a38d3b5349"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Function that saves 'data' to a json file. Constructs a file path is dir_name is provided.\n",
    "def json_save(data, file_name, dir_name=''):\n",
    "    dir_name = [dir_name] if ((type(dir_name) != list) and (dir_name)) else dir_name\n",
    "    assert type(file_name) == str\n",
    "    file_name = file_name + '.json' if not file_name.endswith('.json') else file_name\n",
    "    full_path = os.path.join(*dir_name, file_name)\n",
    "    with open(full_path, 'w') as fp:\n",
    "        json.dump(data, fp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T15:42:11.632036800Z",
     "start_time": "2024-01-04T15:42:11.625500700Z"
    }
   },
   "id": "fece8a5cc62541c3"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    result_parent_path = os.path.join(current_directory, config[\"save_location\"])\n",
    "    if not os.path.exists(result_parent_path):\n",
    "        os.makedirs(result_parent_path)\n",
    "    result_path = os.path.join(result_parent_path, config[\"hardware_device\"])\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    save_path = os.path.join(config[\"save_location\"], config[\"hardware_device\"])\n",
    "        \n",
    "    # Check if a cuda device is available\n",
    "    if torch.cuda.is_available():\n",
    "        print('CUDA device available')\n",
    "        torch.set_default_dtype(torch.cuda.FloatTensor)\n",
    "        torch.cuda.set_device(0)\n",
    "        cuda = 1\n",
    "    else:\n",
    "        print('CUDA device not available')\n",
    "        cuda = 0\n",
    "        \n",
    "        \n",
    "    # Create the LSTM network with the specified configuration\n",
    "    network = SimpleLSTM(input_size=config[\"input_size\"], \n",
    "                         output_size=config[\"output_size\"], \n",
    "                         hidden_size=config[\"hidden_size\"], \n",
    "                         skip=config[\"skip_con\"])\n",
    "        \n",
    "    # Set up training optimiser + scheduler + loss fcns and training info tracker\n",
    "    optimiser = torch.optim.Adam(network.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=5, verbose=True)\n",
    "    loss_functions = LossWrapper(config[\"loss_fcns\"])\n",
    "    train_track = TrainTrack()\n",
    "\n",
    "    dataset = DataSet(data_dir='Data')\n",
    "\n",
    "    dataset.create_subset('train', frame_len=22050)\n",
    "    dataset.load_file(os.path.join('train', config[\"hardware_device\"]), 'train')\n",
    "    \n",
    "    dataset.create_subset('val')\n",
    "    dataset.load_file(os.path.join('val', config[\"hardware_device\"]), 'val')\n",
    "    \n",
    "        # If training is restarting, this will ensure the previously elapsed training time is added to the total\n",
    "    init_time = time.time() - start_time + train_track['total_time']*3600\n",
    "    # Set network save_state flag to true, so when the save_model method is called the network weights are saved\n",
    "    network.save_state = True\n",
    "\n",
    "    for epoch in range(train_track['current_epoch'] + 1, config[\"epochs\"] + 1):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        ep_st_time = time.time()\n",
    "\n",
    "        # Run 1 epoch of training,\n",
    "        epoch_loss = network.train_epoch(dataset.subsets['train'].data['input'][0],\n",
    "                                         dataset.subsets['train'].data['target'][0],\n",
    "                                         loss_functions, optimiser, config['batch_size'], config['init_length'], config['up_fr'])\n",
    "\n",
    "        if epoch % config['validation_f'] == 0:\n",
    "            val_ep_st_time = time.time()\n",
    "            val_output, val_loss = network.process_data(dataset.subsets['val'].data['input'][0],\n",
    "                                             dataset.subsets['val'].data['target'][0], loss_functions, config[\"val_chunk\"])\n",
    "            scheduler.step(val_loss)\n",
    "            print(\"Val loss:\", val_loss)\n",
    "            if val_loss < train_track['best_val_loss']:\n",
    "                network.save_model('model_best', save_path)\n",
    "                write(os.path.join(save_path, \"best_val_out.wav\"),\n",
    "                      dataset.subsets['val'].fs, val_output.cpu().numpy()[:, 0, 0])\n",
    "                \n",
    "            train_track.val_epoch_update(val_loss.item(), val_ep_st_time, time.time())\n",
    "\n",
    "        print('current learning rate: ' + str(optimiser.param_groups[0]['lr']))\n",
    "        train_track.train_epoch_update(epoch_loss.item(), ep_st_time, time.time(), init_time, epoch)\n",
    "        # write loss to the tensorboard (just for recording purposes)\n",
    "        network.save_model('model', save_path)\n",
    "        json_save(train_track, 'training_stats', save_path)        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:24:54.180312Z",
     "start_time": "2024-01-04T18:24:54.160306100Z"
    }
   },
   "id": "cca470a4e37d92cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device not available\n",
      "Epoch:  1\n"
     ]
    }
   ],
   "source": [
    "trainConfig = {\n",
    "    \"input_size\": 1, # Number of channels\n",
    "    \"output_size\": 1, # Number of channels\n",
    "    \"skip_con\": 0, # is there a skip connection for the input to the output\n",
    "    \"epochs\": 500,\n",
    "    \"batch_size\": 8,\n",
    "    \"init_length\": 200, # Number of sequence samples to process before starting weight updates\n",
    "    \"up_fr\": 1000, # For recurrent models, number of samples to run in between updating network weights\n",
    "    \"validation_f\": 1, # Validation Frequency (in epochs)\n",
    "    \"val_chunk\": 1000, #Number of sequence samples to process in n each chunk of validation\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"hidden_size\": 32,\n",
    "    \"loss_fcns\": {\"ESR\": 0.75, \"DC\": 0.25},\n",
    "    \"hardware_device\": \"ht1\",\n",
    "    \"save_location\": \"Results\",\n",
    "    \"export_json\": 1,\n",
    "    \"export_torchscript\": 1\n",
    "}\n",
    "\n",
    "train(trainConfig)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-04T18:50:38.460293700Z"
    }
   },
   "id": "ddc03fcf5a2209c3"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]]])\n",
      "tensor([[[0.1637]],\n",
      "\n",
      "        [[0.1720]],\n",
      "\n",
      "        [[0.1719]],\n",
      "\n",
      "        [[0.1682]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.jit.load(\"Results/ht1/model_best_scripted.pt\")\n",
    "\n",
    "# Example input\n",
    "sequence_length = 100  # adjust as per your model's training\n",
    "batch_size = 1         # can be set to 1 for testing individual sequences\n",
    "input_size = loaded_model.input_size  # should match the model's expected input size\n",
    "\n",
    "test_input = torch.zeros(4, 1, 1)\n",
    "\n",
    "print(test_input)\n",
    "test_output = loaded_model(test_input)\n",
    "print(test_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T18:46:57.808327500Z",
     "start_time": "2024-01-04T18:46:57.751068400Z"
    }
   },
   "id": "fdd84631b080cd4d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
