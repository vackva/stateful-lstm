{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "from contextlib import nullcontext\n",
    "import torchinfo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.561764600Z",
     "start_time": "2024-01-20T19:11:46.543325600Z"
    }
   },
   "id": "d80fc465400cc842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "*This code is identically to the code found in the tensorflow training notebook*\n",
    "\n",
    "Basics:\n",
    " - provide a input and target audio file in the config\n",
    " - This will create 3 folders [train, val, test] in /Data which will be used for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f57b51c451debb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def prepare_training_data(config):\n",
    "    in_rate, in_data = wavfile.read(config[\"input_audio_path\"])\n",
    "    out_rate, out_data = wavfile.read(config[\"target_audio_path\"])\n",
    "    \n",
    "    if len(in_data) != len(out_data):\n",
    "        print(\"input and target files have different lengths\")\n",
    "        sys.exit()\n",
    "      \n",
    "    if len(in_data.shape) > 1 or len(out_data.shape) > 1:\n",
    "        print(\"expected mono files\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Convert PCM16 to FP32\n",
    "    if in_data.dtype == \"int16\":\n",
    "        in_data = in_data / 32767\n",
    "        print(\"In data converted from PCM16 to FP32\")\n",
    "    if out_data.dtype == \"int16\":\n",
    "        out_data = out_data / 32767\n",
    "        print(\"Out data converted from PCM16 to FP32\")    \n",
    "\n",
    "    clean_data = in_data.astype(np.float32).flatten()\n",
    "    target_data = out_data.astype(np.float32).flatten()\n",
    "\n",
    "    # Split the data on a twenty percent mod\n",
    "    in_train, out_train, in_val, out_val = slice_on_mod(clean_data, target_data)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-input.wav\", in_train)\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-target.wav\", out_train)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-target.wav\", out_val)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-target.wav\", out_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.581861100Z",
     "start_time": "2024-01-20T19:11:50.567733400Z"
    }
   },
   "id": "1cdc8701863f502b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def slice_on_mod(input_data, target_data, mod=5):\n",
    "    # Split the data on a modulus.\n",
    "\n",
    "    # Type cast to an integer the modulus\n",
    "    mod = int(mod)\n",
    "\n",
    "    # Split the data into 100 pieces\n",
    "    input_split = np.array_split(input_data, 100)\n",
    "    target_split = np.array_split(target_data, 100)\n",
    "\n",
    "    val_input_data = []\n",
    "    val_target_data = []\n",
    "    # Traverse the range of the indexes of the input signal reversed and pop every 5th for val\n",
    "    for i in reversed(range(len(input_split))):\n",
    "        if i % mod == 0:\n",
    "            # Store the validation data\n",
    "            val_input_data.append(input_split[i])\n",
    "            val_target_data.append(target_split[i])\n",
    "            # Remove the validation data from training\n",
    "            input_split.pop(i)\n",
    "            target_split.pop(i)\n",
    "\n",
    "    # Flatten val_data down to one dimension and concatenate\n",
    "    val_input_data = np.concatenate(val_input_data)\n",
    "    val_target_data = np.concatenate(val_target_data)\n",
    "\n",
    "    # Concatenate back together\n",
    "    training_input_data = np.concatenate(input_split)\n",
    "    training_target_data = np.concatenate(target_split)\n",
    "    return training_input_data, training_target_data, val_input_data, val_target_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.624507500Z",
     "start_time": "2024-01-20T19:11:50.577348Z"
    }
   },
   "id": "d2db02127a930a4c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    directory = os.path.dirname(name)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.624507500Z",
     "start_time": "2024-01-20T19:11:50.598923100Z"
    }
   },
   "id": "8ac5a30c5ce341b1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In data converted from PCM16 to FP32\n",
      "Out data converted from PCM16 to FP32\n"
     ]
    }
   ],
   "source": [
    "importConfig = {\n",
    "    \"input_audio_path\": \"TrainingData/flanger-input.wav\",\n",
    "    \"target_audio_path\": \"TrainingData/flanger-target.wav\",\n",
    "    \"output_path\": \"Data\",\n",
    "    \"name\": \"flanger\"\n",
    "}\n",
    "\n",
    "prepare_training_data(importConfig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.711771400Z",
     "start_time": "2024-01-20T19:11:50.612001500Z"
    }
   },
   "id": "c7cc0b6f364b5692"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23b0f24e4fb42f7d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# converts numpy audio into frames, and creates a torch tensor from them, frame_len = 0 just converts to a torch tensor\n",
    "def framify(audio, frame_len):\n",
    "    # If audio is mono, add a dummy dimension, so the same operations can be applied to mono/multichannel audio\n",
    "    audio = np.expand_dims(audio, 1) if len(audio.shape) == 1 else audio\n",
    "    # Calculate the number of segments the training data will be split into in frame_len is not 0\n",
    "    seg_num = math.floor(audio.shape[0] / frame_len) if frame_len else 1\n",
    "    # If no frame_len is provided, set frame_len to be equal to length of the input audio\n",
    "    frame_len = audio.shape[0] if not frame_len else frame_len\n",
    "    # Find the number of channels\n",
    "    channels = audio.shape[1]\n",
    "    # Initialise tensor matrices\n",
    "    dataset = torch.empty((frame_len, seg_num, channels))\n",
    "    # Load the audio for the training set\n",
    "    for i in range(seg_num):\n",
    "        dataset[:, i, :] = torch.from_numpy(audio[i * frame_len:(i + 1) * frame_len, :])\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.725497300Z",
     "start_time": "2024-01-20T19:11:50.660945800Z"
    }
   },
   "id": "866ea4d69fd9ea3a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data_dir='../Dataset/', extensions=('input', 'target')):\n",
    "        self.extensions = extensions if extensions else ['']\n",
    "        self.subsets = {}\n",
    "        assert type(data_dir) == str, \"data_dir should be string,not %r\" % {type(data_dir)}\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    # add a subset called 'name', desired 'frame_len' is given in seconds, or 0 for just one long frame\n",
    "    def create_subset(self, name, frame_len=0):\n",
    "        assert type(name) == str, \"data subset name must be a string, not %r\" %{type(name)}\n",
    "        assert not (name in self.subsets), \"subset %r already exists\" %name\n",
    "        self.subsets[name] = SubSet(frame_len)\n",
    "\n",
    "    # load a file of 'filename' into existing subset/s 'set_names', split fractionally as specified by 'splits',\n",
    "    # if 'cond_val' is provided the conditioning value will be saved along with the frames of the loaded data\n",
    "    def load_file(self, filename, set_names='train', splits=None, cond_val=None):\n",
    "        # Assertions and checks\n",
    "        if type(set_names) == str:\n",
    "            set_names = [set_names]\n",
    "        assert len(set_names) == 1 or len(set_names) == len(splits), \"number of subset names must equal number of \" \\\n",
    "                                                                     \"split markers\"\n",
    "        assert [self.subsets.get(each) for each in set_names], \"set_names contains subsets that don't exist yet\"\n",
    "\n",
    "        # Load each of the 'extensions'\n",
    "        for i, ext in enumerate(self.extensions):\n",
    "            try:\n",
    "                file_loc = os.path.join(self.data_dir, filename + '-' + ext)\n",
    "                file_loc = file_loc + '.wav' if not file_loc.endswith('.wav') else file_loc\n",
    "                np_data = wavfile.read(file_loc)\n",
    "            except FileNotFoundError:\n",
    "                print([\"File Not Found At: \" + self.data_dir + filename])\n",
    "                return\n",
    "\n",
    "            raw_audio = np_data[1].astype(np.float32)\n",
    "\n",
    "            if len(set_names) == 1:\n",
    "                self.subsets[set_names[0]].add_data(np_data[0], raw_audio, ext, cond_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.727500700Z",
     "start_time": "2024-01-20T19:11:50.681351Z"
    }
   },
   "id": "118b8c57055147f5"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# The SubSet class holds a subset of data,\n",
    "# frame_len sets the length of audio per frame (in s), if set to 0 a single frame is used instead\n",
    "class SubSet:\n",
    "    def __init__(self, frame_len):\n",
    "        self.data = {}\n",
    "        self.frame_len = frame_len\n",
    "        self.conditioning = None\n",
    "        self.fs = None\n",
    "\n",
    "    # Add 'audio' data, in the data dictionary at the key 'ext', if cond_val is provided save the cond_val of each frame\n",
    "    def add_data(self, fs, audio, ext, cond_val):\n",
    "        if not self.fs:\n",
    "            self.fs = fs\n",
    "        assert self.fs == fs, \"data with different sample rate provided to subset\"\n",
    "        # if no 'ext' is provided, all the subsets data will be stored at the 'data' key of the 'data' dict\n",
    "        ext = 'data' if not ext else ext\n",
    "        # Frame the data and optionally create a tensor of the conditioning values of each frame\n",
    "        framed_data = framify(audio, self.frame_len)\n",
    "\n",
    "        try:\n",
    "            # Convert data from tuple to list and concatenate new data onto the data tensor\n",
    "            data = list(self.data[ext])\n",
    "            self.data[ext] = (torch.cat((data[0], framed_data), 1),)\n",
    "        # If this is the first data to be loaded into the subset, create the data and cond_data tuples\n",
    "        except KeyError:\n",
    "            self.data[ext] = (framed_data,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.729117300Z",
     "start_time": "2024-01-20T19:11:50.706041800Z"
    }
   },
   "id": "1a11652a5b27af35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f2ed6f6e582365"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class StatefulLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=0, bias_fl=True):\n",
    "        super(StatefulLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.skip = skip\n",
    "        \n",
    "        # Create dictionary of possible block types\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size)\n",
    "        self.lin = nn.Linear(hidden_size, output_size, bias=bias_fl)\n",
    "        self.hidden = (torch.zeros(self.input_size, 1, self.hidden_size),\n",
    "                       torch.zeros(self.input_size, 1, self.hidden_size))\n",
    "\n",
    "    # Origin forward function \n",
    "    def forward(self, x):    \n",
    "        if self.skip:\n",
    "            # save the residual for the skip connection\n",
    "            res = x[:, :, 0:self.skip]\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x) + res\n",
    "        else:\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            x = self.lin(x)\n",
    "            return x\n",
    "\n",
    "    # detach hidden state, this resets gradient tracking on the hidden state\n",
    "    def detach_hidden(self):\n",
    "        if self.hidden.__class__ == tuple:\n",
    "            self.hidden = tuple([h.clone().detach() for h in self.hidden])\n",
    "        else:\n",
    "            self.hidden = self.hidden.clone().detach()\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.hidden = (torch.zeros(self.input_size, batch_size, self.hidden_size), \n",
    "                       torch.zeros(self.input_size, batch_size, self.hidden_size))\n",
    "\n",
    "    # train_epoch runs one epoch of training\n",
    "    def train_epoch(self, input_data, target_data, loss_fcn, optim, bs, init_len=200, up_fr=1000):\n",
    "\n",
    "        # shuffle the segments at the start of the epoch\n",
    "        shuffle = torch.randperm(input_data.shape[1])\n",
    "    \n",
    "        self.reset_hidden(bs)\n",
    "        \n",
    "        ep_loss = 0\n",
    "        #Iterate over batches of {bs} batches\n",
    "        for batch_i in range(math.ceil(shuffle.shape[0] / bs)):\n",
    "            # Load batch of shuffled segments\n",
    "            input_batch = input_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "            target_batch = target_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "\n",
    "            # Initialise network hidden state by processing some samples then zero the gradient buffers\n",
    "            # For training processing eine Anfangssequenz, damit ein brauchbarer hidden state vorliegt\n",
    "            # Training startet erst nach! einem eingelaufen hidden state\n",
    "            out = self(input_batch[0:init_len, :, :])\n",
    "                        \n",
    "            self.zero_grad()\n",
    "\n",
    "            # Choose the starting index for processing the rest of the batch sequence, in chunks of args.up_fr\n",
    "            start_i = init_len\n",
    "            batch_loss = 0\n",
    "            # Iterate over the remaining samples in the mini batch\n",
    "            for k in range(math.ceil((input_batch.shape[0] - init_len) / up_fr)):\n",
    "                # Process input batch with neural network                \n",
    "                output = self(input_batch[start_i:start_i + up_fr, :, :])\n",
    "\n",
    "                # Calculate loss and update network parameters\n",
    "                loss = loss_fcn(output, target_batch[start_i:start_i + up_fr, :, :])\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                print(f\"loss: {loss}\")\n",
    "\n",
    "                # Set the network hidden state, to detach it from the computation graph\n",
    "                # Die Gradienteninformation die einhergeht mit dem hidden state ist connected zu dem\n",
    "                # Computiaonal Graf von den vorherigen outputs. Da wir immer nur den letzten hidden state\n",
    "                # zur berechnung brauchen und den state auch nicht updaten wollen - wird er hier detached\n",
    "                # detached = wir löchen die Gradienteninformation \n",
    "                # https://discuss.pytorch.org/t/stupid-question-why-do-you-have-to-detach-the-hidden-state-of-lstms-but-not-the-hidden-state-of-a-linear-network/95089/3                \n",
    "                self.detach_hidden()\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Update the start index for the next iteration and add the loss to the batch_loss total\n",
    "                start_i += up_fr\n",
    "                batch_loss += loss\n",
    "\n",
    "            # Add the average batch loss to the epoch loss and reset the hidden states to zeros\n",
    "            ep_loss += batch_loss / (k + 1)\n",
    "        \n",
    "        return ep_loss / (batch_i + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.828827600Z",
     "start_time": "2024-01-20T19:11:50.727500700Z"
    }
   },
   "id": "860763dc7078f73d"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ESRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESRLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.add(target, -output)\n",
    "        loss = torch.pow(loss, 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss\n",
    "class DCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.pow(torch.add(torch.mean(target, 0), -torch.mean(output, 0)), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss      \n",
    "class LossWrapper(nn.Module):\n",
    "    def __init__(self, losses):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        loss_dict = {'ESR': ESRLoss(), 'DC': DCLoss()}\n",
    "\n",
    "        loss_functions = [[loss_dict[key], value] for key, value in losses.items()]\n",
    "\n",
    "        self.loss_functions = tuple([items[0] for items in loss_functions])\n",
    "        try:\n",
    "            self.loss_factors = tuple(torch.Tensor([items[1] for items in loss_functions]))\n",
    "        except IndexError:\n",
    "            self.loss_factors = torch.ones(len(self.loss_functions))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = 0\n",
    "        for i, losses in enumerate(self.loss_functions):\n",
    "            loss += torch.mul(losses(output, target), self.loss_factors[i])\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.830883100Z",
     "start_time": "2024-01-20T19:11:50.755376900Z"
    }
   },
   "id": "cbd60e7e671135d4"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"input_size\": 1, # Number of channels\n",
    "    \"output_size\": 1, # Number of channels\n",
    "    \"skip_con\": 0, # is there a skip connection for the input to the output\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 16,\n",
    "    \"init_length\": 200, # Number of sequence samples to process before starting weight updates\n",
    "    \"up_fr\": 1000, # For recurrent models, number of samples to run in between updating network weights\n",
    "    \"validation_f\": 1, # Validation Frequency (in epochs)\n",
    "    \"val_chunk\": 1000, #Number of sequence samples to process in n each chunk of validation\n",
    "    \"learning_rate\": 0.0005, \n",
    "    \"hidden_size\": 32,\n",
    "    \"loss_fcns\": {\"ESR\": 0.75, \"DC\": 0.25},\n",
    "    \"hardware_device\": \"flanger\",\n",
    "    \"save_location\": \"Results-PyTorch\",\n",
    "    \"export_json\": 1,\n",
    "    \"export_torchscript\": 1,\n",
    "    \"stateful_lstm\": 1\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:11:50.830883100Z",
     "start_time": "2024-01-20T19:11:50.765601900Z"
    }
   },
   "id": "cca470a4e37d92cd"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device not available\n",
      "Creating Stateful LSTM\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "StatefulLSTM                             [1, 1, 1]                 --\n",
      "├─LSTM: 1-1                              [1, 1, 32]                4,480\n",
      "├─Linear: 1-2                            [1, 1, 1]                 33\n",
      "==========================================================================================\n",
      "Total params: 4,513\n",
      "Trainable params: 4,513\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n",
      "Epoch:  1\n",
      "loss: 375.7005615234375\n",
      "loss: 318.1959228515625\n",
      "loss: 335.1663818359375\n",
      "loss: 330.03240966796875\n",
      "loss: 315.40576171875\n",
      "loss: 296.50946044921875\n",
      "loss: 230.29856872558594\n",
      "loss: 217.5951690673828\n",
      "loss: 206.94898986816406\n",
      "loss: 213.00393676757812\n",
      "loss: 250.07321166992188\n",
      "loss: 254.5784149169922\n",
      "loss: 261.7413330078125\n",
      "loss: 203.62075805664062\n",
      "loss: 223.8120574951172\n",
      "loss: 250.25856018066406\n",
      "loss: 227.7099151611328\n",
      "loss: 221.15005493164062\n",
      "loss: 216.1476593017578\n",
      "loss: 213.92074584960938\n",
      "loss: 169.50746154785156\n",
      "loss: 122.0167465209961\n",
      "Epoch loss: tensor(247.8815, grad_fn=<DivBackward0>)\n",
      "Epoch:  2\n",
      "loss: 182.51856994628906\n",
      "loss: 144.30984497070312\n",
      "loss: 165.19171142578125\n",
      "loss: 172.23760986328125\n",
      "loss: 167.9459228515625\n",
      "loss: 167.2461700439453\n",
      "loss: 110.9896469116211\n",
      "loss: 107.70079803466797\n",
      "loss: 102.49164581298828\n",
      "loss: 127.28936767578125\n",
      "loss: 164.5700225830078\n",
      "loss: 183.05081176757812\n",
      "loss: 193.57205200195312\n",
      "loss: 152.55105590820312\n",
      "loss: 173.41783142089844\n",
      "loss: 206.61351013183594\n",
      "loss: 192.01011657714844\n",
      "loss: 189.81007385253906\n",
      "loss: 187.1705322265625\n",
      "loss: 190.21240234375\n",
      "loss: 149.2078857421875\n",
      "loss: 111.16197204589844\n",
      "Epoch loss: tensor(160.9668, grad_fn=<DivBackward0>)\n",
      "Epoch:  3\n",
      "loss: 168.1807861328125\n",
      "loss: 135.6641387939453\n",
      "loss: 154.8802032470703\n",
      "loss: 162.97625732421875\n",
      "loss: 159.528564453125\n",
      "loss: 159.7960968017578\n",
      "loss: 106.28376007080078\n",
      "loss: 102.51311492919922\n",
      "loss: 97.21406555175781\n",
      "loss: 120.20983123779297\n",
      "loss: 155.33851623535156\n",
      "loss: 171.5697021484375\n",
      "loss: 182.25376892089844\n",
      "loss: 141.3253173828125\n",
      "loss: 161.76109313964844\n",
      "loss: 193.636474609375\n",
      "loss: 178.65977478027344\n",
      "loss: 176.61253356933594\n",
      "loss: 174.69349670410156\n",
      "loss: 177.546142578125\n",
      "loss: 139.62489318847656\n",
      "loss: 102.7851791381836\n",
      "Epoch loss: tensor(151.0479, grad_fn=<DivBackward0>)\n",
      "Epoch:  4\n",
      "loss: 157.26513671875\n",
      "loss: 125.54866027832031\n",
      "loss: 144.72528076171875\n",
      "loss: 152.77908325195312\n",
      "loss: 149.73089599609375\n",
      "loss: 149.35513305664062\n",
      "loss: 99.81238555908203\n",
      "loss: 95.98528289794922\n",
      "loss: 91.04037475585938\n",
      "loss: 112.55409240722656\n",
      "loss: 144.90953063964844\n",
      "loss: 159.61196899414062\n",
      "loss: 169.8394775390625\n",
      "loss: 131.40408325195312\n",
      "loss: 149.78013610839844\n",
      "loss: 180.0408172607422\n",
      "loss: 165.27011108398438\n",
      "loss: 162.8932342529297\n",
      "loss: 160.46800231933594\n",
      "loss: 163.44908142089844\n",
      "loss: 128.2360382080078\n",
      "loss: 94.51551818847656\n",
      "Epoch loss: tensor(140.4188, grad_fn=<DivBackward0>)\n",
      "Epoch:  5\n",
      "loss: 143.83047485351562\n",
      "loss: 114.89764404296875\n",
      "loss: 131.8350067138672\n",
      "loss: 139.6302032470703\n",
      "loss: 136.606201171875\n",
      "loss: 136.04493713378906\n",
      "loss: 91.63744354248047\n",
      "loss: 87.42497253417969\n",
      "loss: 82.36522674560547\n",
      "loss: 101.97492980957031\n",
      "loss: 130.09194946289062\n",
      "loss: 142.5012969970703\n",
      "loss: 152.19558715820312\n",
      "loss: 116.82282257080078\n",
      "loss: 132.58349609375\n",
      "loss: 159.97756958007812\n",
      "loss: 144.96070861816406\n",
      "loss: 142.2473907470703\n",
      "loss: 139.2269744873047\n",
      "loss: 142.24856567382812\n",
      "loss: 111.68565368652344\n",
      "loss: 81.70060729980469\n",
      "Epoch loss: tensor(125.5677, grad_fn=<DivBackward0>)\n",
      "Epoch:  6\n",
      "loss: 122.75830841064453\n",
      "loss: 97.68335723876953\n",
      "loss: 111.24226379394531\n",
      "loss: 118.66737365722656\n",
      "loss: 114.86451721191406\n",
      "loss: 113.50692749023438\n",
      "loss: 78.5131607055664\n",
      "loss: 73.84708404541016\n",
      "loss: 68.13095092773438\n",
      "loss: 84.10897827148438\n",
      "loss: 104.50576782226562\n",
      "loss: 112.85491943359375\n",
      "loss: 121.01708221435547\n",
      "loss: 91.66114807128906\n",
      "loss: 102.00404357910156\n",
      "loss: 121.35677337646484\n",
      "loss: 104.89231872558594\n",
      "loss: 102.4668197631836\n",
      "loss: 97.64728546142578\n",
      "loss: 102.823974609375\n",
      "loss: 82.888916015625\n",
      "loss: 58.985252380371094\n",
      "Epoch loss: tensor(99.3831, grad_fn=<DivBackward0>)\n",
      "Epoch:  7\n",
      "loss: 79.73308563232422\n",
      "loss: 65.30016326904297\n",
      "loss: 71.5906753540039\n",
      "loss: 83.21176147460938\n",
      "loss: 74.3604965209961\n",
      "loss: 77.14928436279297\n",
      "loss: 65.73722076416016\n",
      "loss: 69.9303970336914\n",
      "loss: 56.232154846191406\n",
      "loss: 66.84416198730469\n",
      "loss: 83.47024536132812\n",
      "loss: 100.58963012695312\n",
      "loss: 103.31977844238281\n",
      "loss: 76.68968200683594\n",
      "loss: 85.81996154785156\n",
      "loss: 95.7701644897461\n",
      "loss: 79.16944122314453\n",
      "loss: 81.51449584960938\n",
      "loss: 79.65104675292969\n",
      "loss: 91.19066619873047\n",
      "loss: 78.96404266357422\n",
      "loss: 57.888248443603516\n",
      "Epoch loss: tensor(78.3694, grad_fn=<DivBackward0>)\n",
      "Epoch:  8\n",
      "loss: 75.11685943603516\n",
      "loss: 64.45250701904297\n",
      "loss: 71.11833953857422\n",
      "loss: 82.13825988769531\n",
      "loss: 75.53340911865234\n",
      "loss: 76.94961547851562\n",
      "loss: 64.18000030517578\n",
      "loss: 61.97853469848633\n",
      "loss: 53.01097869873047\n",
      "loss: 63.60638427734375\n",
      "loss: 75.30702209472656\n",
      "loss: 84.89204406738281\n",
      "loss: 94.09400177001953\n",
      "loss: 71.64231872558594\n",
      "loss: 78.35215759277344\n",
      "loss: 85.41842651367188\n",
      "loss: 69.65384674072266\n",
      "loss: 74.40605163574219\n",
      "loss: 73.76070404052734\n",
      "loss: 85.34136962890625\n",
      "loss: 75.42103576660156\n",
      "loss: 54.67011260986328\n",
      "Epoch loss: tensor(73.2293, grad_fn=<DivBackward0>)\n",
      "Epoch:  9\n",
      "loss: 67.85423278808594\n",
      "loss: 59.86161804199219\n",
      "loss: 67.08511352539062\n",
      "loss: 79.41947937011719\n",
      "loss: 70.13479614257812\n",
      "loss: 73.90279388427734\n",
      "loss: 65.9305419921875\n",
      "loss: 64.92203521728516\n",
      "loss: 51.72992706298828\n",
      "loss: 62.372703552246094\n",
      "loss: 72.42236328125\n",
      "loss: 82.52971649169922\n",
      "loss: 90.67330932617188\n",
      "loss: 68.84176635742188\n",
      "loss: 74.79867553710938\n",
      "loss: 81.525390625\n",
      "loss: 66.38092041015625\n",
      "loss: 71.09432220458984\n",
      "loss: 70.91974639892578\n",
      "loss: 82.83831787109375\n",
      "loss: 73.8130111694336\n",
      "loss: 53.6063346862793\n",
      "Epoch loss: tensor(70.5753, grad_fn=<DivBackward0>)\n",
      "Epoch:  10\n",
      "loss: 66.13910675048828\n",
      "loss: 58.439544677734375\n",
      "loss: 66.0252685546875\n",
      "loss: 76.9693603515625\n",
      "loss: 68.79838562011719\n",
      "loss: 72.59970092773438\n",
      "loss: 65.1568832397461\n",
      "loss: 62.948734283447266\n",
      "loss: 50.569602966308594\n",
      "loss: 60.609352111816406\n",
      "loss: 69.34891510009766\n",
      "loss: 78.40071105957031\n",
      "loss: 86.71357727050781\n",
      "loss: 66.18441009521484\n",
      "loss: 71.58897399902344\n",
      "loss: 77.96585845947266\n",
      "loss: 63.288265228271484\n",
      "loss: 67.28545379638672\n",
      "loss: 67.23613739013672\n",
      "loss: 79.2954330444336\n",
      "loss: 72.40784454345703\n",
      "loss: 53.068580627441406\n",
      "Epoch loss: tensor(68.2291, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "result_parent_path = os.path.join(current_directory, config[\"save_location\"])\n",
    "os.makedirs(result_parent_path, exist_ok=True)\n",
    "result_path = os.path.join(result_parent_path, config[\"hardware_device\"])\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(config[\"save_location\"], config[\"hardware_device\"])\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device available')\n",
    "    torch.set_default_dtype(torch.cuda.FloatTensor)\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    print('CUDA device not available')\n",
    "\n",
    "print(\"Creating Stateful LSTM\")\n",
    "network = StatefulLSTM(input_size=config[\"input_size\"], \n",
    "                       output_size=config[\"output_size\"], \n",
    "                       hidden_size=config[\"hidden_size\"], \n",
    "                       skip=config[\"skip_con\"])\n",
    "\n",
    "optimiser = torch.optim.Adam(network.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-4)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=5, verbose=True)\n",
    "loss_functions = torch.nn.MSELoss(reduction='sum')\n",
    "#loss_functions = LossWrapper(config[\"loss_fcns\"])\n",
    "\n",
    "summary = torchinfo.summary(network, (1, 1, 1), device=torch.device(\"cpu\"))\n",
    "print(summary)\n",
    "\n",
    "dataset = DataSet(data_dir='Data')\n",
    "dataset.create_subset('train', frame_len=22050)\n",
    "dataset.load_file(os.path.join('train', config[\"hardware_device\"]), 'train')\n",
    "\n",
    "dataset.create_subset('val')\n",
    "dataset.load_file(os.path.join('val', config[\"hardware_device\"]), 'val')\n",
    "\n",
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    # Run 1 epoch of training\n",
    "    epoch_loss = network.train_epoch(dataset.subsets['train'].data['input'][0],\n",
    "                                     dataset.subsets['train'].data['target'][0],\n",
    "                                     loss_functions, optimiser, config['batch_size'], config['init_length'], config['up_fr'])\n",
    "\n",
    "    print(\"Epoch loss:\", epoch_loss)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T19:13:11.667796100Z",
     "start_time": "2024-01-20T19:11:55.834678Z"
    }
   },
   "id": "ddc03fcf5a2209c3"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#network.reset_hidden(1)\n",
    "torch.save(network.state_dict(), \"models/ht1.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T17:59:19.080417600Z",
     "start_time": "2024-01-20T17:59:19.039888200Z"
    }
   },
   "id": "b0b75b4466489f46"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StatefulLSTM(input_size=config[\"input_size\"], \n",
    "                           output_size=config[\"output_size\"], \n",
    "                           hidden_size=config[\"hidden_size\"], \n",
    "                           skip=config[\"skip_con\"])\n",
    "\n",
    "model.load_state_dict(torch.load(\"models/ht1.pth\", map_location=torch.device('cpu')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T18:09:43.401999400Z",
     "start_time": "2024-01-20T18:09:43.353812500Z"
    }
   },
   "id": "c9d058091229b579"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96000, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.subsets['val'].data['input'][0][0:96000,...].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T18:09:45.926663200Z",
     "start_time": "2024-01-20T18:09:45.891457600Z"
    }
   },
   "id": "6e71650307748c8f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "with (torch.no_grad()):\n",
    "    output = model(dataset.subsets['val'].data['input'][0][0:96000,...])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T18:10:15.837653700Z",
     "start_time": "2024-01-20T18:10:05.048396800Z"
    }
   },
   "id": "535228f882ad0aa4"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "write(os.path.join(\"best_val_out.wav\"),\n",
    "                      dataset.subsets['val'].fs, output.cpu().numpy()[:, 0, 0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T18:10:19.276811100Z",
     "start_time": "2024-01-20T18:10:19.259180400Z"
    }
   },
   "id": "2423babc043a6b17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name=\"test\"\n",
    "direc=''\n",
    "\n",
    "# Scripting the model for compatibility with LibTorch\n",
    "model.reset_hidden(1)\n",
    "model_scripted = torch.jit.script(model)\n",
    "# # Saving the scripted model\n",
    "scripted_model_file = file_name + \"_scripted.pt\"\n",
    "if direc:\n",
    "     scripted_model_file = os.path.join(direc, scripted_model_file)\n",
    "model_scripted.save(scripted_model_file)\n",
    "\n",
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(1, 1, 1).to(torch.device(\"cpu\"))\n",
    "onnx_model_file = file_name + \".onnx\"\n",
    "if direc:\n",
    "     onnx_model_file = os.path.join(direc, onnx_model_file)\n",
    "\n",
    "torch.onnx.export(model=self,\n",
    "                  args=example,\n",
    "                  f=onnx_model_file,\n",
    "                  export_params=True,\n",
    "                  opset_version=13,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-20T17:25:44.728206900Z"
    }
   },
   "id": "3a2e02f6a75e6ecc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loaded_model = torch.jit.load(\"Results-PyTorch/flanger/model_best_scripted.pt\")\n",
    "\n",
    "# Example input\n",
    "sequence_length = 100  # adjust as per your model's training\n",
    "batch_size = 1         # can be set to 1 for testing individual sequences\n",
    "input_size = loaded_model.input_size  # should match the model's expected input size\n",
    "\n",
    "test_input = torch.zeros(1, 1, 1)\n",
    "\n",
    "print(test_input)\n",
    "test_output = loaded_model(test_input)\n",
    "print(test_output)\n",
    "test_output = loaded_model(test_input)\n",
    "print(test_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-20T17:25:44.730365500Z"
    }
   },
   "id": "fdd84631b080cd4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"Results-PyTorch/flanger/model_best.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Create an input tensor with shape (1, 1, 1) filled with zeros\n",
    "test_input = np.zeros((1, 1, 1), dtype=np.float32)\n",
    "\n",
    "# Run inference on the ONNX model\n",
    "ort_inputs = {\"input\": test_input}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "ort_outputs2 = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Print the output\n",
    "print(ort_outputs)\n",
    "# Print the output\n",
    "print(ort_outputs2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-20T17:25:44.732304300Z"
    }
   },
   "id": "a4a5336bb61971ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
