{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: torch in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (0.16.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: onnx in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: onnxscript in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (0.1.0.dev20240106)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from onnx) (4.25.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from onnxruntime) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from onnxruntime) (23.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\valentin ackva\\documents\\code\\stateful-lstm\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy scipy torch torchvision torchaudio torchinfo onnx onnxscript onnxruntime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T18:08:06.961807Z",
     "start_time": "2024-01-15T18:08:01.478708200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "from contextlib import nullcontext\n",
    "import torchinfo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:13.387123Z",
     "start_time": "2024-01-17T18:22:08.443785900Z"
    }
   },
   "id": "d80fc465400cc842"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "*This code is identically to the code found in the tensorflow training notebook*\n",
    "\n",
    "Basics:\n",
    " - provide a input and target audio file in the config\n",
    " - This will create 3 folders [train, val, test] in /Data which will be used for training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4f57b51c451debb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def prepare_training_data(config):\n",
    "    in_rate, in_data = wavfile.read(config[\"input_audio_path\"])\n",
    "    out_rate, out_data = wavfile.read(config[\"target_audio_path\"])\n",
    "    \n",
    "    if len(in_data) != len(out_data):\n",
    "        print(\"input and target files have different lengths\")\n",
    "        sys.exit()\n",
    "      \n",
    "    if len(in_data.shape) > 1 or len(out_data.shape) > 1:\n",
    "        print(\"expected mono files\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Convert PCM16 to FP32\n",
    "    if in_data.dtype == \"int16\":\n",
    "        in_data = in_data / 32767\n",
    "        print(\"In data converted from PCM16 to FP32\")\n",
    "    if out_data.dtype == \"int16\":\n",
    "        out_data = out_data / 32767\n",
    "        print(\"Out data converted from PCM16 to FP32\")    \n",
    "\n",
    "    clean_data = in_data.astype(np.float32).flatten()\n",
    "    target_data = out_data.astype(np.float32).flatten()\n",
    "\n",
    "    # Split the data on a twenty percent mod\n",
    "    in_train, out_train, in_val, out_val = slice_on_mod(clean_data, target_data)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-input.wav\", in_train)\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-target.wav\", out_train)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-target.wav\", out_val)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-target.wav\", out_val)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:15.455265200Z",
     "start_time": "2024-01-17T18:22:15.433775600Z"
    }
   },
   "id": "1cdc8701863f502b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def slice_on_mod(input_data, target_data, mod=5):\n",
    "    # Split the data on a modulus.\n",
    "\n",
    "    # Type cast to an integer the modulus\n",
    "    mod = int(mod)\n",
    "\n",
    "    # Split the data into 100 pieces\n",
    "    input_split = np.array_split(input_data, 100)\n",
    "    target_split = np.array_split(target_data, 100)\n",
    "\n",
    "    val_input_data = []\n",
    "    val_target_data = []\n",
    "    # Traverse the range of the indexes of the input signal reversed and pop every 5th for val\n",
    "    for i in reversed(range(len(input_split))):\n",
    "        if i % mod == 0:\n",
    "            # Store the validation data\n",
    "            val_input_data.append(input_split[i])\n",
    "            val_target_data.append(target_split[i])\n",
    "            # Remove the validation data from training\n",
    "            input_split.pop(i)\n",
    "            target_split.pop(i)\n",
    "\n",
    "    # Flatten val_data down to one dimension and concatenate\n",
    "    val_input_data = np.concatenate(val_input_data)\n",
    "    val_target_data = np.concatenate(val_target_data)\n",
    "\n",
    "    # Concatenate back together\n",
    "    training_input_data = np.concatenate(input_split)\n",
    "    training_target_data = np.concatenate(target_split)\n",
    "    return training_input_data, training_target_data, val_input_data, val_target_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:17.301453400Z",
     "start_time": "2024-01-17T18:22:17.281377300Z"
    }
   },
   "id": "d2db02127a930a4c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    directory = os.path.dirname(name)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:18.767299400Z",
     "start_time": "2024-01-17T18:22:18.739648500Z"
    }
   },
   "id": "8ac5a30c5ce341b1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In data converted from PCM16 to FP32\n",
      "Out data converted from PCM16 to FP32\n"
     ]
    }
   ],
   "source": [
    "importConfig = {\n",
    "    \"input_audio_path\": \"TrainingData/flanger-input.wav\",\n",
    "    \"target_audio_path\": \"TrainingData/flanger-target.wav\",\n",
    "    \"output_path\": \"Data\",\n",
    "    \"name\": \"flanger\"\n",
    "}\n",
    "\n",
    "prepare_training_data(importConfig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:20.107002900Z",
     "start_time": "2024-01-17T18:22:20.034804100Z"
    }
   },
   "id": "c7cc0b6f364b5692"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f2ed6f6e582365"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class StatefulLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=1, bias_fl=True, num_layers=1):\n",
    "        super(StatefulLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Create dictionary of possible block types\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.lin = nn.Linear(hidden_size, output_size, bias=bias_fl)\n",
    "        self.bias_fl = bias_fl\n",
    "        self.skip = skip\n",
    "        self.save_state = True\n",
    "        self.hidden = (torch.zeros(self.input_size, 1, self.hidden_size),\n",
    "                       torch.zeros(self.input_size, 1, self.hidden_size))\n",
    "\n",
    "    # Origin forward function \n",
    "    def forward(self, x):    \n",
    "        if self.skip:\n",
    "            # save the residual for the skip connection\n",
    "            res = x[:, :, 0:self.skip]\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x) + res\n",
    "        else:\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x)\n",
    "\n",
    "    # detach hidden state, this resets gradient tracking on the hidden state\n",
    "    def detach_hidden(self):\n",
    "        if self.hidden.__class__ == tuple:\n",
    "            self.hidden = tuple([h.clone().detach() for h in self.hidden])\n",
    "        else:\n",
    "            self.hidden = self.hidden.clone().detach()\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.hidden = (torch.zeros(self.input_size, batch_size, self.hidden_size), \n",
    "                       torch.zeros(self.input_size, batch_size, self.hidden_size))\n",
    "\n",
    "    # This functions saves the model and all its paraemters to a json file, so it can be loaded by a JUCE plugin\n",
    "    def save_model(self, file_name, direc=''):\n",
    "        model_data = {'model_data': {'model': 'SimpleRNN', 'input_size': self.lstm.input_size, 'skip': self.skip,\n",
    "                                     'output_size': self.lin.out_features, 'unit_type': self.lstm._get_name(),\n",
    "                                     'num_layers': self.lstm.num_layers, 'hidden_size': self.lstm.hidden_size,\n",
    "                                     'bias_fl': self.bias_fl}}\n",
    "\n",
    "        if self.save_state:\n",
    "            model_state = self.state_dict()\n",
    "            for each in model_state:\n",
    "                model_state[each] = model_state[each].tolist()\n",
    "            model_data['state_dict'] = model_state\n",
    "\n",
    "        json_save(model_data, file_name, direc)\n",
    "\n",
    "        # Scripting the model for compatibility with LibTorch\n",
    "        \n",
    "        self.reset_hidden(1)\n",
    "        \n",
    "        model_scripted = torch.jit.script(self)\n",
    "        # \n",
    "        # # Saving the scripted model\n",
    "        scripted_model_file = file_name + \"_scripted.pt\"\n",
    "        if direc:\n",
    "             scripted_model_file = os.path.join(direc, scripted_model_file)\n",
    "        model_scripted.save(scripted_model_file)\n",
    "        \n",
    "        # An example input you would normally provide to your model's forward() method.\n",
    "        example = torch.rand(1, 1, 1).to(torch.device(\"cpu\"))\n",
    "        onnx_model_file = file_name + \".onnx\"\n",
    "        if direc:\n",
    "             onnx_model_file = os.path.join(direc, onnx_model_file)\n",
    "        \n",
    "        torch.onnx.export(model=self,\n",
    "                          args=example,\n",
    "                          f=onnx_model_file,\n",
    "                          export_params=True,\n",
    "                          opset_version=13,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'])\n",
    "\n",
    "    # train_epoch runs one epoch of training\n",
    "    def train_epoch(self, input_data, target_data, loss_fcn, optim, bs, init_len=200, up_fr=1000):\n",
    "        \n",
    "        print (\"train_epoch\")\n",
    "        print (input_data.shape)\n",
    "        # shuffle the segments at the start of the epoch\n",
    "        shuffle = torch.randperm(input_data.shape[1])\n",
    "        \n",
    "        print(shuffle)\n",
    "\n",
    "        self.reset_hidden(bs)\n",
    "        \n",
    "        # Iterate over the batches\n",
    "        ep_loss = 0\n",
    "        for batch_i in range(math.ceil(shuffle.shape[0] / bs)):\n",
    "            # Load batch of shuffled segments\n",
    "            input_batch = input_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "            target_batch = target_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "\n",
    "            print(\"Iterate over the batches\")\n",
    "            print(input_data.shape)\n",
    "            print(target_batch.shape)\n",
    "\n",
    "            # Initialise network hidden state by processing some samples then zero the gradient buffers\n",
    "            self(input_batch[0:init_len, :, :])\n",
    "            self.zero_grad()\n",
    "\n",
    "            # Choose the starting index for processing the rest of the batch sequence, in chunks of args.up_fr\n",
    "            start_i = init_len\n",
    "            batch_loss = 0\n",
    "            # Iterate over the remaining samples in the mini batch\n",
    "            for k in range(math.ceil((input_batch.shape[0] - init_len) / up_fr)):\n",
    "                # Process input batch with neural network\n",
    "                output = self(input_batch[start_i:start_i + up_fr, :, :])\n",
    "\n",
    "                # Calculate loss and update network parameters\n",
    "                loss = loss_fcn(output, target_batch[start_i:start_i + up_fr, :, :])\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Set the network hidden state, to detach it from the computation graph\n",
    "                self.detach_hidden()\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Update the start index for the next iteration and add the loss to the batch_loss total\n",
    "                start_i += up_fr\n",
    "                batch_loss += loss\n",
    "\n",
    "            # Add the average batch loss to the epoch loss and reset the hidden states to zeros\n",
    "            ep_loss += batch_loss / (k + 1)\n",
    "        return ep_loss / (batch_i + 1)\n",
    "\n",
    "    # only proc processes a the input data and calculates the loss, optionally grad can be tracked or not\n",
    "    def process_data(self, input_data, target_data, loss_fcn, chunk, grad=False, validate=False):\n",
    "        with (torch.no_grad() if not grad else nullcontext()):\n",
    "            self.reset_hidden(input_data.shape[1])\n",
    "            output = torch.empty_like(target_data)\n",
    "            for l in range(int(output.size()[0] / chunk)):\n",
    "                if validate:\n",
    "                    output[l * chunk:(l + 1) * chunk] = input_data[l * chunk:(l + 1) * chunk]\n",
    "                else:\n",
    "                    output[l * chunk:(l + 1) * chunk] = self(input_data[l * chunk:(l + 1) * chunk])\n",
    "                    self.detach_hidden()\n",
    "            # If the data set doesn't divide evenly into the chunk length, process the remainder\n",
    "            if not (output.size()[0] / chunk).is_integer():\n",
    "                if validate:\n",
    "                    output[(l + 1) * chunk:-1] = input_data[(l + 1) * chunk:-1]                    \n",
    "                else:\n",
    "                    output[(l + 1) * chunk:-1] = self(input_data[(l + 1) * chunk:-1])\n",
    "            loss = loss_fcn(output, target_data)\n",
    "        return output, loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:42:09.377753400Z",
     "start_time": "2024-01-17T18:42:09.340054100Z"
    }
   },
   "id": "860763dc7078f73d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class StatelessLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=1, bias_fl=True, num_layers=1):\n",
    "        super(StatelessLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.lin = nn.Linear(hidden_size, output_size, bias=bias_fl)\n",
    "        self.bias_fl = bias_fl\n",
    "        self.skip = skip\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.skip:\n",
    "            res = x[:, :, 0:self.skip]\n",
    "            x, _ = self.lstm(x)\n",
    "            return self.lin(x) + res\n",
    "        else:\n",
    "            x, _ = self.lstm(x)\n",
    "            return self.lin(x)\n",
    "        \n",
    "    def reset_hidden(self, batch_size):\n",
    "        # dummy function\n",
    "        return \n",
    "\n",
    "    # This functions saves the model and all its paraemters to a json file, so it can be loaded by a JUCE plugin\n",
    "    def save_model(self, file_name, direc=''):\n",
    "        model_data = {'model_data': {'model': 'SimpleRNN', 'input_size': self.lstm.input_size, 'skip': self.skip,\n",
    "                                     'output_size': self.lin.out_features, 'unit_type': self.lstm._get_name(),\n",
    "                                     'num_layers': self.lstm.num_layers, 'hidden_size': self.lstm.hidden_size,\n",
    "                                     'bias_fl': self.bias_fl}}\n",
    "\n",
    "        if self.save_state:\n",
    "            model_state = self.state_dict()\n",
    "            for each in model_state:\n",
    "                model_state[each] = model_state[each].tolist()\n",
    "            model_data['state_dict'] = model_state\n",
    "\n",
    "        json_save(model_data, file_name, direc)\n",
    "\n",
    "        # Scripting the model for compatibility with LibTorch\n",
    "                \n",
    "        model_scripted = torch.jit.script(self)\n",
    "        # \n",
    "        # # Saving the scripted model\n",
    "        scripted_model_file = file_name + \"_scripted.pt\"\n",
    "        if direc:\n",
    "             scripted_model_file = os.path.join(direc, scripted_model_file)\n",
    "        model_scripted.save(scripted_model_file)\n",
    "        \n",
    "        # An example input you would normally provide to your model's forward() method.\n",
    "        example = torch.rand(1, 1, 1).to(torch.device(\"cpu\"))\n",
    "        onnx_model_file = file_name + \".onnx\"\n",
    "        if direc:\n",
    "             onnx_model_file = os.path.join(direc, onnx_model_file)\n",
    "        \n",
    "        torch.onnx.export(model=self,\n",
    "                          args=example,\n",
    "                          f=onnx_model_file,\n",
    "                          export_params=True,\n",
    "                          opset_version=13,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'])\n",
    "\n",
    "    # train_epoch runs one epoch of training\n",
    "    def train_epoch(self, input_data, target_data, loss_fcn, optim, bs, init_len=200, up_fr=1000):\n",
    "        # shuffle the segments at the start of the epoch\n",
    "        shuffle = torch.randperm(input_data.shape[1])\n",
    "        \n",
    "        # Iterate over the batches\n",
    "        ep_loss = 0\n",
    "        for batch_i in range(math.ceil(shuffle.shape[0] / bs)):\n",
    "            # Load batch of shuffled segments\n",
    "            input_batch = input_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "            target_batch = target_data[:, shuffle[batch_i * bs:(batch_i + 1) * bs], :]\n",
    "\n",
    "            # Initialise network hidden state by processing some samples then zero the gradient buffers\n",
    "            self(input_batch[0:init_len, :, :])\n",
    "            self.zero_grad()\n",
    "\n",
    "            # Choose the starting index for processing the rest of the batch sequence, in chunks of args.up_fr\n",
    "            start_i = init_len\n",
    "            batch_loss = 0\n",
    "            # Iterate over the remaining samples in the mini batch\n",
    "            for k in range(math.ceil((input_batch.shape[0] - init_len) / up_fr)):\n",
    "                # Process input batch with neural network\n",
    "                output = self(input_batch[start_i:start_i + up_fr, :, :])\n",
    "\n",
    "                # Calculate loss and update network parameters\n",
    "                loss = loss_fcn(output, target_batch[start_i:start_i + up_fr, :, :])\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Set the network hidden state, to detach it from the computation graph\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Update the start index for the next iteration and add the loss to the batch_loss total\n",
    "                start_i += up_fr\n",
    "                batch_loss += loss\n",
    "\n",
    "            # Add the average batch loss to the epoch loss and reset the hidden states to zeros\n",
    "            ep_loss += batch_loss / (k + 1)\n",
    "        return ep_loss / (batch_i + 1)\n",
    "\n",
    "    # only proc processes a the input data and calculates the loss, optionally grad can be tracked or not\n",
    "    def process_data(self, input_data, target_data, loss_fcn, chunk, grad=False, validate=False):\n",
    "        with (torch.no_grad() if not grad else nullcontext()):\n",
    "            output = torch.empty_like(target_data)\n",
    "            for l in range(int(output.size()[0] / chunk)):\n",
    "                if validate:\n",
    "                    output[l * chunk:(l + 1) * chunk] = input_data[l * chunk:(l + 1) * chunk]\n",
    "                else:\n",
    "                    output[l * chunk:(l + 1) * chunk] = self(input_data[l * chunk:(l + 1) * chunk])\n",
    "            # If the data set doesn't divide evenly into the chunk length, process the remainder\n",
    "            if not (output.size()[0] / chunk).is_integer():\n",
    "                if validate:\n",
    "                    output[(l + 1) * chunk:-1] = input_data[(l + 1) * chunk:-1]                    \n",
    "                else:\n",
    "                    output[(l + 1) * chunk:-1] = self(input_data[(l + 1) * chunk:-1])\n",
    "            loss = loss_fcn(output, target_data)\n",
    "        return output, loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:28.314923300Z",
     "start_time": "2024-01-17T18:22:28.280688900Z"
    }
   },
   "id": "28870ce8b36c3881"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LossWrapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;66;03m# Print the loss to check if it's computed\u001B[39;00m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 14\u001B[0m \u001B[43mvalidate_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m, in \u001B[0;36mvalidate_process_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate_process_data\u001B[39m():\n\u001B[0;32m      2\u001B[0m     lstm \u001B[38;5;241m=\u001B[39m StatefulLSTM()\n\u001B[1;32m----> 3\u001B[0m     loss_functions \u001B[38;5;241m=\u001B[39m \u001B[43mLossWrapper\u001B[49m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mESR\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.75\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDC\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.25\u001B[39m})\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# (sequence_length, batch_size, output_size)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     input_data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m20\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'LossWrapper' is not defined"
     ]
    }
   ],
   "source": [
    "def validate_process_data():\n",
    "    lstm = StatefulLSTM()\n",
    "    loss_functions = LossWrapper({\"ESR\": 0.75, \"DC\": 0.25})\n",
    "    \n",
    "    # (sequence_length, batch_size, output_size)\n",
    "    input_data = torch.ones(10, 20, 1)\n",
    "    target_data = torch.ones(10, 20, 1)\n",
    "    chunk_size = 5\n",
    "    output, loss = lstm.process_data(input_data, target_data, loss_functions, chunk_size, validate=True)\n",
    "    \n",
    "    # Print the loss to check if it's computed\n",
    "    print(f'Loss: {loss.item()}')\n",
    "    \n",
    "validate_process_data()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T14:57:56.901247300Z",
     "start_time": "2024-01-14T14:57:56.725211900Z"
    }
   },
   "id": "db68245b1f036b43"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed for batch size 1 and skip 0\n",
      "Test passed for batch size 1 and skip 1\n"
     ]
    }
   ],
   "source": [
    "def test_forward():\n",
    "    # Test parameters\n",
    "    input_size = 1\n",
    "    output_size = 1\n",
    "    hidden_size = 32\n",
    "    batch_sizes = [1]  # Different batch sizes to test\n",
    "    sequence_length = 100  # Length of the input sequences\n",
    "\n",
    "    for skip in [0, 1]:  # Test with and without skip connection\n",
    "        model = StatefulLSTM(input_size=input_size, output_size=output_size, hidden_size=hidden_size, skip=skip)\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            # Generate random input tensor\n",
    "            # Shape: (sequence_length, batch_size, input_size)\n",
    "            x = torch.rand(sequence_length, batch_size, input_size)\n",
    "\n",
    "            # Test the forward function\n",
    "            output = model(x)\n",
    "\n",
    "            # Check output shape\n",
    "            expected_shape = (sequence_length, batch_size, output_size)\n",
    "            assert output.shape == expected_shape, f\"Output shape mismatch: {output.shape} != {expected_shape}\"\n",
    "\n",
    "            print(f\"Test passed for batch size {batch_size} and skip {skip}\")\n",
    "\n",
    "# Run the test\n",
    "test_forward()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T14:58:22.810274800Z",
     "start_time": "2024-01-14T14:58:22.577034Z"
    }
   },
   "id": "2aba545987adace4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class ESRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESRLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.add(target, -output)\n",
    "        loss = torch.pow(loss, 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss\n",
    "class DCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.pow(torch.add(torch.mean(target, 0), -torch.mean(output, 0)), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss      \n",
    "class LossWrapper(nn.Module):\n",
    "    def __init__(self, losses):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        loss_dict = {'ESR': ESRLoss(), 'DC': DCLoss()}\n",
    "\n",
    "        loss_functions = [[loss_dict[key], value] for key, value in losses.items()]\n",
    "\n",
    "        self.loss_functions = tuple([items[0] for items in loss_functions])\n",
    "        try:\n",
    "            self.loss_factors = tuple(torch.Tensor([items[1] for items in loss_functions]))\n",
    "        except IndexError:\n",
    "            self.loss_factors = torch.ones(len(self.loss_functions))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = 0\n",
    "        for i, losses in enumerate(self.loss_functions):\n",
    "            loss += torch.mul(losses(output, target), self.loss_factors[i])\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:36.702804900Z",
     "start_time": "2024-01-17T18:22:36.678221Z"
    }
   },
   "id": "cbd60e7e671135d4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Wrapper: 0.9999899864196777\n"
     ]
    }
   ],
   "source": [
    "def test_loss_functions():\n",
    "    loss_wrapper = LossWrapper({'ESR': 0.6, 'DC': 0.4})  # Example weights\n",
    "\n",
    "    # Generate example output and target data\n",
    "    output = torch.zeros(10, 5)  # Example output data\n",
    "    target = torch.ones(10, 5)  # Example target data\n",
    "\n",
    "    loss_wrapper_val = loss_wrapper(output, target)\n",
    "\n",
    "    # Print the loss\n",
    "    print(f'Loss Wrapper: {loss_wrapper_val.item()}')\n",
    "\n",
    "# Run the test\n",
    "test_loss_functions()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-14T14:58:25.491806500Z",
     "start_time": "2024-01-14T14:58:25.459998400Z"
    }
   },
   "id": "a56f46358c971fa6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# converts numpy audio into frames, and creates a torch tensor from them, frame_len = 0 just converts to a torch tensor\n",
    "def framify(audio, frame_len):\n",
    "    # If audio is mono, add a dummy dimension, so the same operations can be applied to mono/multichannel audio\n",
    "    audio = np.expand_dims(audio, 1) if len(audio.shape) == 1 else audio\n",
    "    # Calculate the number of segments the training data will be split into in frame_len is not 0\n",
    "    seg_num = math.floor(audio.shape[0] / frame_len) if frame_len else 1\n",
    "    # If no frame_len is provided, set frame_len to be equal to length of the input audio\n",
    "    frame_len = audio.shape[0] if not frame_len else frame_len\n",
    "    # Find the number of channels\n",
    "    channels = audio.shape[1]\n",
    "    # Initialise tensor matrices\n",
    "    dataset = torch.empty((frame_len, seg_num, channels))\n",
    "    # Load the audio for the training set\n",
    "    for i in range(seg_num):\n",
    "        dataset[:, i, :] = torch.from_numpy(audio[i * frame_len:(i + 1) * frame_len, :])\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:39.027517900Z",
     "start_time": "2024-01-17T18:22:39.003231100Z"
    }
   },
   "id": "9eda68cb093d2ea1"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono Framed Shape: torch.Size([4000, 4, 1])\n",
      "Stereo Framed Shape: torch.Size([4000, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "def test_framify():\n",
    "    # Generate example mono audio data (1D array)\n",
    "    mono_audio = np.random.rand(16000)  # Example: 1 second of audio at 16kHz\n",
    "\n",
    "    # Generate example stereo audio data (2D array)\n",
    "    stereo_audio = np.random.rand(16000, 2)  # Example: stereo audio\n",
    "\n",
    "    # Define frame length\n",
    "    frame_len = 4000  # Example frame length\n",
    "\n",
    "    # Call framify for mono audio\n",
    "    mono_framed = framify(mono_audio, frame_len)\n",
    "    # Call framify for stereo audio\n",
    "    stereo_framed = framify(stereo_audio, frame_len)\n",
    "\n",
    "    # Check the shapes of the outputs\n",
    "    print(\"Mono Framed Shape:\", mono_framed.shape)  # Expected: (4000, 4, 1)\n",
    "    print(\"Stereo Framed Shape:\", stereo_framed.shape)  # Expected: (4000, 4, 2)\n",
    "    \n",
    "# Run the test\n",
    "test_framify()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T18:08:47.359672800Z",
     "start_time": "2024-01-15T18:08:47.315628300Z"
    }
   },
   "id": "713cc9d6c1c2c3f2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data_dir='../Dataset/', extensions=('input', 'target')):\n",
    "        self.extensions = extensions if extensions else ['']\n",
    "        self.subsets = {}\n",
    "        assert type(data_dir) == str, \"data_dir should be string,not %r\" % {type(data_dir)}\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    # add a subset called 'name', desired 'frame_len' is given in seconds, or 0 for just one long frame\n",
    "    def create_subset(self, name, frame_len=0):\n",
    "        assert type(name) == str, \"data subset name must be a string, not %r\" %{type(name)}\n",
    "        assert not (name in self.subsets), \"subset %r already exists\" %name\n",
    "        self.subsets[name] = SubSet(frame_len)\n",
    "\n",
    "    # load a file of 'filename' into existing subset/s 'set_names', split fractionally as specified by 'splits',\n",
    "    # if 'cond_val' is provided the conditioning value will be saved along with the frames of the loaded data\n",
    "    def load_file(self, filename, set_names='train', splits=None, cond_val=None):\n",
    "        # Assertions and checks\n",
    "        if type(set_names) == str:\n",
    "            set_names = [set_names]\n",
    "        assert len(set_names) == 1 or len(set_names) == len(splits), \"number of subset names must equal number of \" \\\n",
    "                                                                     \"split markers\"\n",
    "        assert [self.subsets.get(each) for each in set_names], \"set_names contains subsets that don't exist yet\"\n",
    "\n",
    "        # Load each of the 'extensions'\n",
    "        for i, ext in enumerate(self.extensions):\n",
    "            try:\n",
    "                file_loc = os.path.join(self.data_dir, filename + '-' + ext)\n",
    "                file_loc = file_loc + '.wav' if not file_loc.endswith('.wav') else file_loc\n",
    "                np_data = wavfile.read(file_loc)\n",
    "            except FileNotFoundError:\n",
    "                print([\"File Not Found At: \" + self.data_dir + filename])\n",
    "                return\n",
    "\n",
    "            raw_audio = np_data[1].astype(np.float32)\n",
    "\n",
    "            if len(set_names) == 1:\n",
    "                self.subsets[set_names[0]].add_data(np_data[0], raw_audio, ext, cond_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:43.319004200Z",
     "start_time": "2024-01-17T18:22:43.274745500Z"
    }
   },
   "id": "8ada66f7a022959"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# The SubSet class holds a subset of data,\n",
    "# frame_len sets the length of audio per frame (in s), if set to 0 a single frame is used instead\n",
    "class SubSet:\n",
    "    def __init__(self, frame_len):\n",
    "        self.data = {}\n",
    "        self.cond_data = {}\n",
    "        self.frame_len = frame_len\n",
    "        self.conditioning = None\n",
    "        self.fs = None\n",
    "\n",
    "    # Add 'audio' data, in the data dictionary at the key 'ext', if cond_val is provided save the cond_val of each frame\n",
    "    def add_data(self, fs, audio, ext, cond_val):\n",
    "        if not self.fs:\n",
    "            self.fs = fs\n",
    "        assert self.fs == fs, \"data with different sample rate provided to subset\"\n",
    "        # if no 'ext' is provided, all the subsets data will be stored at the 'data' key of the 'data' dict\n",
    "        ext = 'data' if not ext else ext\n",
    "        # Frame the data and optionally create a tensor of the conditioning values of each frame\n",
    "        framed_data = framify(audio, self.frame_len)\n",
    "        cond_data = cond_val * torch.ones(framed_data.shape[1]) if isinstance(cond_val, (float, int)) else None\n",
    "\n",
    "        try:\n",
    "            # Convert data from tuple to list and concatenate new data onto the data tensor\n",
    "            data = list(self.data[ext])\n",
    "            self.data[ext] = (torch.cat((data[0], framed_data), 1),)\n",
    "            # If cond_val is provided add it to the cond_val tensor, note all frames or no frames must have cond vals\n",
    "            if isinstance(cond_val, (float, int)):\n",
    "                c_data = list(self.cond_data[ext])\n",
    "                self.cond_data[ext] = (torch.cat((c_data[0], cond_data), 0),)\n",
    "        # If this is the first data to be loaded into the subset, create the data and cond_data tuples\n",
    "        except KeyError:\n",
    "            self.data[ext] = (framed_data,)\n",
    "            self.cond_data[ext] = (cond_data,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:45.669865Z",
     "start_time": "2024-01-17T18:22:45.641667300Z"
    }
   },
   "id": "cb8fc7bf9b21f1f5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class TrainTrack(dict):\n",
    "    def __init__(self):\n",
    "        super(TrainTrack, self).__init__()\n",
    "        self.update({'current_epoch': 0, 'training_losses': [], 'validation_losses': [], 'train_av_time': 0.0,\n",
    "                     'val_av_time': 0.0, 'total_time': 0.0, 'best_val_loss': 1e12, 'test_loss': 0})\n",
    "\n",
    "    def restore_data(self, training_info):\n",
    "        self.update(training_info)\n",
    "\n",
    "    def train_epoch_update(self, loss, ep_st_time, ep_end_time, init_time, current_ep):\n",
    "        if self['train_av_time']:\n",
    "            self['train_av_time'] = (self['train_av_time'] + ep_end_time - ep_st_time) / 2\n",
    "        else:\n",
    "            self['train_av_time'] = ep_end_time - ep_st_time\n",
    "        self['training_losses'].append(loss)\n",
    "        self['current_epoch'] = current_ep\n",
    "        self['total_time'] += ((init_time + ep_end_time - ep_st_time)/3600)\n",
    "\n",
    "    def val_epoch_update(self, loss, ep_st_time, ep_end_time):\n",
    "        if self['val_av_time']:\n",
    "            self['val_av_time'] = (self['val_av_time'] + ep_end_time - ep_st_time) / 2\n",
    "        else:\n",
    "            self['val_av_time'] = ep_end_time - ep_st_time\n",
    "        self['validation_losses'].append(loss)\n",
    "        if loss < self['best_val_loss']:\n",
    "            self['best_val_loss'] = loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:47.233396600Z",
     "start_time": "2024-01-17T18:22:47.204785900Z"
    }
   },
   "id": "706337a38d3b5349"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Function that saves 'data' to a json file. Constructs a file path is dir_name is provided.\n",
    "def json_save(data, file_name, dir_name=''):\n",
    "    dir_name = [dir_name] if ((type(dir_name) != list) and (dir_name)) else dir_name\n",
    "    assert type(file_name) == str\n",
    "    file_name = file_name + '.json' if not file_name.endswith('.json') else file_name\n",
    "    full_path = os.path.join(*dir_name, file_name)\n",
    "    with open(full_path, 'w') as fp:\n",
    "        json.dump(data, fp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:50.314734500Z",
     "start_time": "2024-01-17T18:22:50.282111Z"
    }
   },
   "id": "fece8a5cc62541c3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def train(config):\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    result_parent_path = os.path.join(current_directory, config[\"save_location\"])\n",
    "    if not os.path.exists(result_parent_path):\n",
    "        os.makedirs(result_parent_path)\n",
    "    result_path = os.path.join(result_parent_path, config[\"hardware_device\"])\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "\n",
    "    save_path = os.path.join(config[\"save_location\"], config[\"hardware_device\"])\n",
    "        \n",
    "    # Check if a cuda device is available\n",
    "    if torch.cuda.is_available():\n",
    "        print('CUDA device available')\n",
    "        torch.set_default_dtype(torch.cuda.FloatTensor)\n",
    "        torch.cuda.set_device(0)\n",
    "        cuda = 1\n",
    "    else:\n",
    "        print('CUDA device not available')\n",
    "        cuda = 0\n",
    "        \n",
    "        \n",
    "    if int(config[\"stateful_lstm\"]) == 1:\n",
    "        # Create the LSTM network with the specified configuration\n",
    "        print(\"Creating Stateful LSTM\")\n",
    "        network = StatefulLSTM(input_size=config[\"input_size\"], \n",
    "                               output_size=config[\"output_size\"], \n",
    "                               hidden_size=config[\"hidden_size\"], \n",
    "                               skip=config[\"skip_con\"])\n",
    "    else:\n",
    "        print(\"Creating Stateless LSTM\")\n",
    "        network = StatelessLSTM(input_size=config[\"input_size\"], \n",
    "                               output_size=config[\"output_size\"], \n",
    "                               hidden_size=config[\"hidden_size\"], \n",
    "                               skip=config[\"skip_con\"])\n",
    "        \n",
    "    summary = torchinfo.summary(network, (1, 1, 1), device=torch.device(\"cpu\"))\n",
    "    print(summary)\n",
    "        \n",
    "    # Set up training optimiser + scheduler + loss fcns and training info tracker\n",
    "    optimiser = torch.optim.Adam(network.parameters(), lr=config[\"learning_rate\"], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=5, verbose=True)\n",
    "    loss_functions = LossWrapper(config[\"loss_fcns\"])\n",
    "    train_track = TrainTrack()\n",
    "\n",
    "    dataset = DataSet(data_dir='Data')\n",
    "\n",
    "    dataset.create_subset('train', frame_len=22050)\n",
    "    dataset.load_file(os.path.join('train', config[\"hardware_device\"]), 'train')\n",
    "    \n",
    "    dataset.create_subset('val')\n",
    "    dataset.load_file(os.path.join('val', config[\"hardware_device\"]), 'val')\n",
    "    \n",
    "        # If training is restarting, this will ensure the previously elapsed training time is added to the total\n",
    "    init_time = time.time() - start_time + train_track['total_time']*3600\n",
    "    # Set network save_state flag to true, so when the save_model method is called the network weights are saved\n",
    "    network.save_state = True\n",
    "\n",
    "    for epoch in range(train_track['current_epoch'] + 1, config[\"epochs\"] + 1):\n",
    "        print(\"Epoch: \", epoch)\n",
    "        ep_st_time = time.time()\n",
    "\n",
    "        # Run 1 epoch of training,\n",
    "        epoch_loss = network.train_epoch(dataset.subsets['train'].data['input'][0],\n",
    "                                         dataset.subsets['train'].data['target'][0],\n",
    "                                         loss_functions, optimiser, config['batch_size'], config['init_length'], config['up_fr'])\n",
    "\n",
    "        if epoch % config['validation_f'] == 0:\n",
    "            val_ep_st_time = time.time()\n",
    "            val_output, val_loss = network.process_data(dataset.subsets['val'].data['input'][0],\n",
    "                                             dataset.subsets['val'].data['target'][0], loss_functions, config[\"val_chunk\"])\n",
    "            scheduler.step(val_loss)\n",
    "            print(\"Val loss:\", val_loss)\n",
    "            if val_loss < train_track['best_val_loss']:\n",
    "                test_in = torch.zeros(1, 1, 1)\n",
    "                network.reset_hidden(1)\n",
    "                test_out = network(test_in)\n",
    "                test_out2 = network(test_in)\n",
    "                print(\"--\")\n",
    "                print(test_out)\n",
    "                print(test_out2)\n",
    "                network.save_model('model_best', save_path)\n",
    "                write(os.path.join(save_path, \"best_val_out.wav\"),\n",
    "                      dataset.subsets['val'].fs, val_output.cpu().numpy()[:, 0, 0])\n",
    "                \n",
    "            train_track.val_epoch_update(val_loss.item(), val_ep_st_time, time.time())\n",
    "\n",
    "        print('current learning rate: ' + str(optimiser.param_groups[0]['lr']))\n",
    "        train_track.train_epoch_update(epoch_loss.item(), ep_st_time, time.time(), init_time, epoch)\n",
    "        # write loss to the tensorboard (just for recording purposes)\n",
    "        network.save_model('model', save_path)\n",
    "        json_save(train_track, 'training_stats', save_path)        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:22:52.344773Z",
     "start_time": "2024-01-17T18:22:52.303122Z"
    }
   },
   "id": "cca470a4e37d92cd"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device not available\n",
      "Creating Stateful LSTM\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "StatefulLSTM                             [1, 1, 1]                 --\n",
      "├─LSTM: 1-1                              [1, 1, 32]                4,480\n",
      "├─Linear: 1-2                            [1, 1, 1]                 33\n",
      "==========================================================================================\n",
      "Total params: 4,513\n",
      "Trainable params: 4,513\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n",
      "Epoch:  1\n",
      "train_epoch\n",
      "torch.Size([22050, 16, 1])\n",
      "tensor([ 5, 10,  6,  7, 14, 11,  1,  3,  2, 13,  9,  0, 12, 15,  8,  4])\n",
      "Iterate over the batches\n",
      "torch.Size([22050, 16, 1])\n",
      "torch.Size([22050, 16, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 21\u001B[0m\n\u001B[0;32m      1\u001B[0m trainConfig \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m, \u001B[38;5;66;03m# Number of channels\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m, \u001B[38;5;66;03m# Number of channels\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstateful_lstm\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     19\u001B[0m }\n\u001B[1;32m---> 21\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainConfig\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[14], line 66\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(config)\u001B[0m\n\u001B[0;32m     63\u001B[0m ep_st_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# Run 1 epoch of training,\u001B[39;00m\n\u001B[1;32m---> 66\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubsets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubsets\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtarget\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mloss_functions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minit_length\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mup_fr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m config[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation_f\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     71\u001B[0m     val_ep_st_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "Cell \u001B[1;32mIn[19], line 117\u001B[0m, in \u001B[0;36mStatefulLSTM.train_epoch\u001B[1;34m(self, input_data, target_data, loss_fcn, optim, bs, init_len, up_fr)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;66;03m# Calculate loss and update network parameters\u001B[39;00m\n\u001B[0;32m    116\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fcn(output, target_batch[start_i:start_i \u001B[38;5;241m+\u001B[39m up_fr, :, :])\n\u001B[1;32m--> 117\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m optim\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    120\u001B[0m \u001B[38;5;66;03m# Set the network hidden state, to detach it from the computation graph\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Code\\stateful-lstm\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainConfig = {\n",
    "    \"input_size\": 1, # Number of channels\n",
    "    \"output_size\": 1, # Number of channels\n",
    "    \"skip_con\": 0, # is there a skip connection for the input to the output\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"init_length\": 200, # Number of sequence samples to process before starting weight updates\n",
    "    \"up_fr\": 1000, # For recurrent models, number of samples to run in between updating network weights\n",
    "    \"validation_f\": 1, # Validation Frequency (in epochs)\n",
    "    \"val_chunk\": 1000, #Number of sequence samples to process in n each chunk of validation\n",
    "    \"learning_rate\": 0.0005, \n",
    "    \"hidden_size\": 32,\n",
    "    \"loss_fcns\": {\"ESR\": 0.75, \"DC\": 0.25},\n",
    "    \"hardware_device\": \"flanger\",\n",
    "    \"save_location\": \"Results-PyTorch\",\n",
    "    \"export_json\": 1,\n",
    "    \"export_torchscript\": 1,\n",
    "    \"stateful_lstm\": 1\n",
    "}\n",
    "\n",
    "train(trainConfig)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T18:42:24.432808600Z",
     "start_time": "2024-01-17T18:42:19.698261Z"
    }
   },
   "id": "ddc03fcf5a2209c3"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.]]])\n",
      "tensor([[[-0.0735]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0735]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = torch.jit.load(\"Results-PyTorch/flanger/model_best_scripted.pt\")\n",
    "\n",
    "# Example input\n",
    "sequence_length = 100  # adjust as per your model's training\n",
    "batch_size = 1         # can be set to 1 for testing individual sequences\n",
    "input_size = loaded_model.input_size  # should match the model's expected input size\n",
    "\n",
    "test_input = torch.zeros(1, 1, 1)\n",
    "\n",
    "print(test_input)\n",
    "test_output = loaded_model(test_input)\n",
    "print(test_output)\n",
    "test_output = loaded_model(test_input)\n",
    "print(test_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T19:35:15.888021400Z",
     "start_time": "2024-01-06T19:35:15.816834200Z"
    }
   },
   "id": "fdd84631b080cd4d"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[-0.07354225]]], dtype=float32)]\n",
      "[array([[[-0.07354225]]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"Results-PyTorch/flanger/model_best.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Create an input tensor with shape (1, 1, 1) filled with zeros\n",
    "test_input = np.zeros((1, 1, 1), dtype=np.float32)\n",
    "\n",
    "# Run inference on the ONNX model\n",
    "ort_inputs = {\"input\": test_input}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "ort_outputs2 = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Print the output\n",
    "print(ort_outputs)\n",
    "# Print the output\n",
    "print(ort_outputs2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-06T19:34:25.293908900Z",
     "start_time": "2024-01-06T19:34:25.197533700Z"
    }
   },
   "id": "a4a5336bb61971ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
