{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fc465400cc842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:22.882990600Z",
     "start_time": "2024-01-21T18:20:08.687914700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import tf2onnx\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f57b51c451debb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc8701863f502b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:22.882990600Z",
     "start_time": "2024-01-21T18:20:22.815455300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_training_data(config):\n",
    "    in_rate, in_data = wavfile.read(config[\"input_audio_path\"])\n",
    "    out_rate, out_data = wavfile.read(config[\"target_audio_path\"])\n",
    "    \n",
    "    if len(in_data) != len(out_data):\n",
    "        print(\"input and target files have different lengths\")\n",
    "        sys.exit()\n",
    "      \n",
    "    if len(in_data.shape) > 1 or len(out_data.shape) > 1:\n",
    "        print(\"expected mono files\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Convert PCM16 to FP32\n",
    "    if in_data.dtype == \"int16\":\n",
    "        in_data = in_data / 32767\n",
    "        print(\"In data converted from PCM16 to FP32\")\n",
    "    if out_data.dtype == \"int16\":\n",
    "        out_data = out_data / 32767\n",
    "        print(\"Out data converted from PCM16 to FP32\")    \n",
    "\n",
    "    clean_data = in_data.astype(np.float32).flatten()\n",
    "    target_data = out_data.astype(np.float32).flatten()\n",
    "\n",
    "    # Split the data on a twenty percent mod\n",
    "    in_train, out_train, in_val, out_val = slice_on_mod(clean_data, target_data)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-input.wav\", in_train)\n",
    "    save_wav(config[\"output_path\"] + \"/train/\" + config[\"name\"] + \"-target.wav\", out_train)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/test/\" + config[\"name\"] + \"-target.wav\", out_val)\n",
    "\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-input.wav\", in_val)\n",
    "    save_wav(config[\"output_path\"] + \"/val/\" + config[\"name\"] + \"-target.wav\", out_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db02127a930a4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:22.882990600Z",
     "start_time": "2024-01-21T18:20:22.826456100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def slice_on_mod(input_data, target_data, mod=5):\n",
    "    # Split the data on a modulus.\n",
    "\n",
    "    # Type cast to an integer the modulus\n",
    "    mod = int(mod)\n",
    "\n",
    "    # Split the data into 100 pieces\n",
    "    input_split = np.array_split(input_data, 100)\n",
    "    target_split = np.array_split(target_data, 100)\n",
    "\n",
    "    val_input_data = []\n",
    "    val_target_data = []\n",
    "    # Traverse the range of the indexes of the input signal reversed and pop every 5th for val\n",
    "    for i in reversed(range(len(input_split))):\n",
    "        if i % mod == 0:\n",
    "            # Store the validation data\n",
    "            val_input_data.append(input_split[i])\n",
    "            val_target_data.append(target_split[i])\n",
    "            # Remove the validation data from training\n",
    "            input_split.pop(i)\n",
    "            target_split.pop(i)\n",
    "\n",
    "    # Flatten val_data down to one dimension and concatenate\n",
    "    val_input_data = np.concatenate(val_input_data)\n",
    "    val_target_data = np.concatenate(val_target_data)\n",
    "\n",
    "    # Concatenate back together\n",
    "    training_input_data = np.concatenate(input_split)\n",
    "    training_target_data = np.concatenate(target_split)\n",
    "    return training_input_data, training_target_data, val_input_data, val_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5a30c5ce341b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:22.882990600Z",
     "start_time": "2024-01-21T18:20:22.841471600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    directory = os.path.dirname(name)\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    wavfile.write(name, 44100, data.flatten().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc0b6f364b5692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:23.434519700Z",
     "start_time": "2024-01-21T18:20:22.857992600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importConfig = {\n",
    "    \"input_audio_path\": \"TrainingData/ts9-input.wav\",\n",
    "    \"target_audio_path\": \"TrainingData/ts9-target.wav\",\n",
    "    \"output_path\": \"Data\",\n",
    "    \"name\": \"ts9\"\n",
    "}\n",
    "\n",
    "prepare_training_data(importConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db2dbe23cb7b1c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026cfbd4fba6c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:23.434519700Z",
     "start_time": "2024-01-21T18:20:23.416522100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data_dir='Data/'):\n",
    "        self.data_dir = data_dir\n",
    "        self.subsets = {}\n",
    "\n",
    "    def create_subset(self, name, frame_len=22050):\n",
    "        self.subsets[name] = {'input': None, 'target': None, 'frame_len': frame_len}\n",
    "\n",
    "    def load_file(self, subset_name, base_filename):\n",
    "        if subset_name not in self.subsets:\n",
    "            raise ValueError(f\"Subset '{subset_name}' does not exist\")\n",
    "\n",
    "        input_file = os.path.join(self.data_dir, f\"{base_filename}-input.wav\")\n",
    "        target_file = os.path.join(self.data_dir, f\"{base_filename}-target.wav\")\n",
    "\n",
    "        try:\n",
    "            self.subsets[subset_name]['input'] = self.load_and_process(input_file, self.subsets[subset_name]['frame_len'])\n",
    "            self.subsets[subset_name]['target'] = self.load_and_process(target_file, self.subsets[subset_name]['frame_len'])\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"File Not Found: {e.filename}\")\n",
    "            return\n",
    "\n",
    "    def load_and_process(self, file_path, frame_len):\n",
    "        sample_rate, data = wavfile.read(file_path)\n",
    "        data = data.astype(np.float32)\n",
    "        return self.framify(data, frame_len)\n",
    "\n",
    "    def framify(self, audio, frame_len):\n",
    "        seg_num = math.ceil(audio.shape[0] / frame_len)\n",
    "        padded_length = seg_num * frame_len\n",
    "        padded_audio = np.pad(audio, (0, padded_length - audio.shape[0]), mode='constant')\n",
    "\n",
    "        reshaped_audio = np.reshape(padded_audio, (seg_num, frame_len, 1))\n",
    "        return tf.convert_to_tensor(reshaped_audio, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2ed6f6e582365",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860763dc7078f73d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:23.601944400Z",
     "start_time": "2024-01-21T18:20:23.441533800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StatefulLSTM(tf.keras.Model):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=1, bias_fl=True, batch_size=4096):\n",
    "        super(StatefulLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.skip = skip\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.lstm = tf.keras.layers.LSTM(units=hidden_size, return_sequences=True, stateful=True, return_state=True, batch_size=batch_size)    \n",
    "        self.dense = tf.keras.layers.Dense(units=output_size, activation=None, batch_size=batch_size, use_bias=bias_fl)\n",
    "        \n",
    "        # Build LSTM before training, because stateful lstm requires information batch size to build static graph\n",
    "        self.lstm.build((batch_size, input_size, 1))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x, _, _ = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.lstm.reset_states()\n",
    "    \n",
    "    def train_epoch(self, input_data, target_data, loss_fcn, optim, bs, init_len=200, up_fr=1000):\n",
    "\n",
    "        # shuffle the segments at the start of the epoch\n",
    "        shuffle = tf.random.shuffle(tf.range(input_data.shape[0]))\n",
    "    \n",
    "        self.reset_hidden(bs)\n",
    "\n",
    "        # Iterate over the batches\n",
    "        ep_loss = 0\n",
    "        for batch_i in range(math.ceil(shuffle.shape[0] / bs)):            \n",
    "\n",
    "            # Use tf.gather to index the tensors\n",
    "            input_batch = tf.gather(input_data, shuffle[batch_i * bs:(batch_i + 1) * bs], axis=0)\n",
    "            target_batch = tf.gather(target_data, shuffle[batch_i * bs:(batch_i + 1) * bs], axis=0)\n",
    "            \n",
    "            # Initialise network hidden state by processing some samples then zero the gradient buffers\n",
    "            # For training processing eine Anfangssequenz, damit ein brauchbarer hidden state vorliegt\n",
    "            # Training startet erst nach! einem eingelaufen hidden state\n",
    "            self(input_batch[:, 0:init_len, :])\n",
    "        \n",
    "            start_i = init_len\n",
    "            batch_loss = 0\n",
    "            # Iterate over the remaining samples in the mini batch\n",
    "            for k in range(math.ceil((input_batch.shape[1] - init_len) / up_fr)):\n",
    "                \n",
    "                with tf.GradientTape() as g:\n",
    "                    # Process input batch with neural network    \n",
    "                    output = self(input_batch[:, start_i:start_i + up_fr, :])\n",
    "                    loss = loss_fcn(output, target_batch[:, start_i:start_i + up_fr, :])\n",
    "                    with g.stop_recording():\n",
    "                        dloss_dw = g.gradient(loss, self.trainable_variables)\n",
    "                        optim.apply_gradients(zip(dloss_dw, self.trainable_variables))\n",
    "                    g.reset()\n",
    "                        \n",
    "                print(f\"loss: {loss}\")\n",
    "\n",
    "                # Update the start index for the next iteration and add the loss to the batch_loss total\n",
    "                start_i += up_fr\n",
    "                batch_loss += loss\n",
    "\n",
    "            # Add the average batch loss to the epoch loss and reset the hidden states to zeros\n",
    "            ep_loss += batch_loss / (k + 1)\n",
    "        \n",
    "        return ep_loss / (batch_i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd60e7e671135d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:20:23.605935500Z",
     "start_time": "2024-01-21T18:20:23.594948700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ESRLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(ESRLoss, self).__init__()\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        energy = tf.reduce_mean(tf.square(y_true)) + self.epsilon\n",
    "        return loss / energy\n",
    "\n",
    "class DCLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(DCLoss, self).__init__()\n",
    "        self.epsilon = 1e-5\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = tf.reduce_mean(tf.square(tf.reduce_mean(y_true, axis=0) - tf.reduce_mean(y_pred, axis=0)))\n",
    "        energy = tf.reduce_mean(tf.square(y_true)) + self.epsilon\n",
    "        return loss / energy\n",
    "\n",
    "class LossWrapper(tf.keras.losses.Loss):\n",
    "    def __init__(self, loss_weights):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        # Map the loss names to their corresponding classes\n",
    "        loss_dict = {'ESR': ESRLoss, 'DC': DCLoss}\n",
    "        # Create instances of the loss functions\n",
    "        self.loss_functions = [loss_dict[key]() for key in [\"ESR\", \"DC\"]]\n",
    "        # Assign the weights\n",
    "        self.loss_factors = [loss_weights[key] for key in [\"ESR\", \"DC\"]]\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        total_loss = 0\n",
    "        for i, loss_function in enumerate(self.loss_functions):\n",
    "            total_loss += loss_function(y_true, y_pred) * self.loss_factors[i]\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c26066b7bc449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:21:30.187220300Z",
     "start_time": "2024-01-21T18:21:30.126220500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"input_size\": 1, # Number of input channels\n",
    "    \"output_size\": 1, # Number of output channels\n",
    "    \"skip_con\": 0, # is there a skip connection for the input to the output\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 16,\n",
    "    \"init_length\": 200, # Number of sequence samples to process before starting weight updates\n",
    "    \"up_fr\": 1000, # For recurrent models, number of samples to run in between updating network weights\n",
    "    \"learning_rate\": 0.0005, \n",
    "    \"hidden_size\": 32,\n",
    "    \"loss_fcns\": {\"ESR\": 0.75, \"DC\": 0.25},\n",
    "    \"hardware_device\": \"ts9\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca470a4e37d92cd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-21T23:12:51.691829200Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f\"These are the physical devices available:\\n{physical_devices}\")\n",
    "\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    print(f\"These are the visible devices:\\n{visible_devices}\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "print(\"Creating Stateful LSTM\")\n",
    "network = StatefulLSTM(input_size=config[\"input_size\"], \n",
    "                       output_size=config[\"output_size\"], \n",
    "                       hidden_size=config[\"hidden_size\"], \n",
    "                       skip=config[\"skip_con\"],\n",
    "                       batch_size=config[\"batch_size\"])\n",
    "\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"], weight_decay=1e-4, epsilon=1e-8)\n",
    "#loss_functions = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "loss_functions = LossWrapper(config[\"loss_fcns\"])\n",
    "\n",
    "network.compile(optimizer=optimiser, loss=loss_functions)\n",
    "network.build((config[\"batch_size\"],1,1))\n",
    "network.summary()        \n",
    "\n",
    "# Usage Example\n",
    "dataset = DataSet()\n",
    "dataset.create_subset('train', frame_len=22050)\n",
    "dataset.load_file('train', os.path.join('train', config[\"hardware_device\"]))\n",
    "\n",
    "dataset.create_subset('val')\n",
    "dataset.load_file('val', os.path.join('val', config[\"hardware_device\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, config[\"epochs\"] + 1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "\n",
    "    epoch_loss = network.train_epoch(dataset.subsets['train']['input'],\n",
    "                                     dataset.subsets['train']['target'],\n",
    "                                     loss_functions,\n",
    "                                     optimiser,\n",
    "                                     config['batch_size'],\n",
    "                                     config['init_length'],\n",
    "                                     config['up_fr'])\n",
    "\n",
    "    print(\"Epoch loss:\", epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc03fcf5a2209c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T23:12:32.907471700Z",
     "start_time": "2024-01-21T23:12:32.767844300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network.save_weights('models/weights')\n",
    "network.save_weights('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc52b05d7df1ea3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:00:58.529427900Z",
     "start_time": "2024-01-21T18:00:58.158983800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference_batch_size = 1\n",
    "\n",
    "inference_model = StatefulLSTM(input_size=1, \n",
    "                               output_size=1,\n",
    "                               hidden_size=32,\n",
    "                               skip=0,\n",
    "                               batch_size=inference_batch_size)\n",
    "\n",
    "input_shape = (inference_batch_size,2048,1)\n",
    "\n",
    "inference_model.build(input_shape)\n",
    "\n",
    "inference_model.load_weights('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6cd479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.subsets['train']['input'].shape)\n",
    "input = dataset.subsets['train']['input']\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.reset_hidden(inference_batch_size)\n",
    "output = inference_model.predict(input, batch_size=inference_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_concat = output[0, :, :]\n",
    "input_concat = input[0, :, :]\n",
    "for i in range(1, output.shape[0]):\n",
    "    output_concat =  np.concatenate((output_concat, output[i, :, :]), axis=0)\n",
    "    input_concat = np.concatenate((input_concat, input[i, :, :]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "write(os.path.join(\"input-tensorflow.wav\"), 44100, input_concat.reshape(-1, 1))\n",
    "write(os.path.join(\"train_out-tensorflow.wav\"), 44100, output_concat.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f7edcd3489db9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:01:01.116143400Z",
     "start_time": "2024-01-21T18:01:00.163057300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape = (1,1,1)\n",
    "test_sequence = tf.zeros(input_shape)\n",
    "\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction = inference_model.predict(test_sequence)\n",
    "print(f\"prediction {prediction}\")\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "prediction = inference_model.predict(test_sequence)\n",
    "print(f\"prediction2 {prediction}\")\n",
    "\n",
    "print(\"test_sequence shape: \", test_sequence.shape)\n",
    "print(\"prediction_2 shape: \", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195b6ac60138cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-21T18:01:15.298992100Z",
     "start_time": "2024-01-21T18:01:08.092721700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = \"model_0\"\n",
    "inference_model.reset_hidden(inference_batch_size)\n",
    "input_shape = [1, 2048, 1]\n",
    "\n",
    "func = tf.function(inference_model).get_concrete_function(\n",
    "    tf.TensorSpec(input_shape, dtype=tf.float32))\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([func], inference_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open(\"models/\"+name+\"/stateful-lstm.tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = [1, 2048, 1]\n",
    "\n",
    "# Define the input shape\n",
    "input_signature = [tf.TensorSpec(input_shape, tf.float32, name='input')]\n",
    "\n",
    "# Convert the model\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(inference_model, input_signature, opset=13)\n",
    "onnx.save(proto=onnx_model, f=\"models/\"+name+\"/\"+\"stateful-lstm-tflite.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = \"models/\"+name+\"/stateful-lstm.tflite\"\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc095210",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], tf.zeros((1, 2048, 1)))\n",
    "interpreter.invoke()\n",
    "prediction = interpreter.get_tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "print(f\"prediction3 {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1994958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model_path = \"models/\"+name+\"/\"+\"stateful-lstm-tflite.onnx\"\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
    "\n",
    "# Create an input tensor with shape (1, 1, 1) filled with zeros\n",
    "test_input = np.zeros((1, 2048, 1), dtype=np.float32)\n",
    "\n",
    "# Run inference on the ONNX model\n",
    "ort_inputs = {\"input\": test_input}\n",
    "ort_outputs = ort_session.run(None, ort_inputs)\n",
    "ort_outputs2 = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Print the output\n",
    "print(ort_outputs)\n",
    "# Print the output\n",
    "print(ort_outputs2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
