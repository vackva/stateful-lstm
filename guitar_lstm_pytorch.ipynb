{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T16:40:04.245402Z",
     "start_time": "2024-01-19T16:40:04.201395700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchinfo\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose computation device (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T16:35:29.880454700Z",
     "start_time": "2024-01-19T16:35:29.837924400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device not available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA device available')\n",
    "    torch.set_default_dtype(torch.cuda.FloatTensor)\n",
    "    torch.cuda.set_device(0)\n",
    "    cuda = 1\n",
    "else:\n",
    "    print('CUDA device not available')\n",
    "    cuda = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T17:36:18.193409100Z",
     "start_time": "2024-01-19T17:36:18.026340700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model with the same name already exists. Please choose a new name.\n"
     ]
    }
   ],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "#\n",
    "name = 'model_1'\n",
    "in_file = 'TrainingData/flanger-input.wav'\n",
    "out_file = 'TrainingData/flanger-target.wav'\n",
    "epochs = 1\n",
    "\n",
    "input_size = 1 \n",
    "batch_size = 16 \n",
    "test_size = 0.2\n",
    "learning_rate = 0.0005 \n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)\n",
    "else:\n",
    "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
    "    exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T17:36:19.965472600Z",
     "start_time": "2024-01-19T17:36:19.918300900Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    sp.io.wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T17:36:25.872465800Z",
     "start_time": "2024-01-19T17:36:21.484860200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape (pre-processing): (352800,)\n",
      "y_training shape (pre-processing): (352800,)\n",
      "X_testing shape (pre-processing): (88200,)\n",
      "y_testing shape (pre-processing): (88200,)\n",
      "X_ordered_training shape: torch.Size([352800, 1, 1])\n",
      "X_ordered_testing shape: torch.Size([88200, 1, 1])\n",
      "y_ordered_training shape: torch.Size([352800, 1])\n",
      "y_ordered_testing shape: torch.Size([88200, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data ###########################################\n",
    "in_rate, in_data = wavfile.read(in_file)\n",
    "out_rate, out_data = wavfile.read(out_file)\n",
    "\n",
    "X_all = in_data.astype(np.float32).flatten()  \n",
    "X_all = normalize(X_all)\n",
    "y_all = out_data.astype(np.float32).flatten() \n",
    "y_all = normalize(y_all)\n",
    "\n",
    "# Get the last 20% of the wav data for testing and the rest for training\n",
    "X_training, X_testing = np.split(X_all, [int(len(X_all) * (1 - test_size))])\n",
    "y_training, y_testing = np.split(y_all, [int(len(y_all) * (1 - test_size))])\n",
    "print(f\"X_training shape (pre-processing): {X_training.shape}\")\n",
    "print(f\"y_training shape (pre-processing): {y_training.shape}\")\n",
    "print(f\"X_testing shape (pre-processing): {X_testing.shape}\")\n",
    "print(f\"y_testing shape (pre-processing): {y_testing.shape}\")\n",
    "\n",
    "# Create a new array where each element is an array of input_size samples in time order\n",
    "indices = np.arange(input_size) + np.arange(len(X_training) - input_size + 1)[:, np.newaxis]\n",
    "indices = torch.from_numpy(indices).long()  # Convert indices to long\n",
    "X_training = torch.from_numpy(X_training)\n",
    "X_ordered_training = torch.zeros((len(indices), input_size), dtype=torch.float32)\n",
    "for i, index in enumerate(indices):\n",
    "    X_ordered_training[i] = torch.gather(X_training, 0, index)\n",
    "X_ordered_training = X_ordered_training.unsqueeze(1)\n",
    "print(f\"X_ordered_training shape: {X_ordered_training.shape}\")\n",
    "\n",
    "indices = np.arange(input_size) + np.arange(len(X_testing) - input_size + 1)[:, np.newaxis]\n",
    "indices = torch.from_numpy(indices).long()  # Convert indices to long\n",
    "X_testing = torch.from_numpy(X_testing)\n",
    "X_ordered_testing = torch.zeros((len(indices), input_size), dtype=torch.float32)\n",
    "for i, index in enumerate(indices):\n",
    "    X_ordered_testing[i] = torch.gather(X_testing, 0, index)\n",
    "X_ordered_testing = X_ordered_testing.unsqueeze(1)\n",
    "print(f\"X_ordered_testing shape: {X_ordered_testing.shape}\")\n",
    "\n",
    "y_ordered_training = y_training[input_size - 1:]\n",
    "y_ordered_training = torch.from_numpy(y_ordered_training).unsqueeze(1)\n",
    "print(f\"y_ordered_training shape: {y_ordered_training.shape}\")\n",
    "\n",
    "y_ordered_testing = y_testing[input_size - 1:]\n",
    "y_ordered_testing = torch.from_numpy(y_ordered_testing).unsqueeze(1)\n",
    "print(f\"y_ordered_testing shape: {y_ordered_testing.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T17:36:31.385799900Z",
     "start_time": "2024-01-19T17:36:31.341577700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Shape of X: torch.Size([16, 1, 1])\n",
      "Shape of y: torch.Size([16, 1]) torch.float32\n",
      "Batch: 0\n",
      "Shape of X: torch.Size([16, 1, 1])\n",
      "Shape of y: torch.Size([16, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "training_dataset = torch.utils.data.TensorDataset(X_ordered_training, y_ordered_training)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch, (X, y) in enumerate(training_dataloader):\n",
    "    print(f\"Batch: {batch}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "testing_dataset = torch.utils.data.TensorDataset(X_ordered_testing, y_ordered_testing)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch, (X, y) in enumerate(testing_dataloader):\n",
    "    print(f\"Batch: {batch}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T16:32:40.170800600Z",
     "start_time": "2024-01-19T16:32:40.116725500Z"
    }
   },
   "outputs": [],
   "source": [
    "class StatefulLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, output_size=1, hidden_size=32, skip=1, bias_fl=True, num_layers=1):\n",
    "        super(StatefulLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Create dictionary of possible block types\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.lin = nn.Linear(hidden_size, output_size, bias=bias_fl)\n",
    "        self.bias_fl = bias_fl\n",
    "        self.skip = skip\n",
    "        self.save_state = True\n",
    "        self.hidden = (torch.zeros(self.input_size, 1, self.hidden_size),\n",
    "                       torch.zeros(self.input_size, 1, self.hidden_size))\n",
    "\n",
    "    # Origin forward function \n",
    "    def forward(self, x):    \n",
    "        if self.skip:\n",
    "            # save the residual for the skip connection\n",
    "            res = x[:, :, 0:self.skip]\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x) + res\n",
    "        else:\n",
    "            x, self.hidden = self.lstm(x, self.hidden)\n",
    "            return self.lin(x)\n",
    "\n",
    "    # detach hidden state, this resets gradient tracking on the hidden state\n",
    "    def detach_hidden(self):\n",
    "        if self.hidden.__class__ == tuple:\n",
    "            self.hidden = tuple([h.clone().detach() for h in self.hidden])\n",
    "        else:\n",
    "            self.hidden = self.hidden.clone().detach()\n",
    "\n",
    "    def reset_hidden(self, batch_size):\n",
    "        self.hidden = (torch.zeros(self.input_size, batch_size, self.hidden_size), \n",
    "                       torch.zeros(self.input_size, batch_size, self.hidden_size))\n",
    "\n",
    "    def save_model(self, file_name, direc=''):\n",
    "        model_data = {'model_data': {'model': 'SimpleRNN', 'input_size': self.lstm.input_size, 'skip': self.skip,\n",
    "                                     'output_size': self.lin.out_features, 'unit_type': self.lstm._get_name(),\n",
    "                                     'num_layers': self.lstm.num_layers, 'hidden_size': self.lstm.hidden_size,\n",
    "                                     'bias_fl': self.bias_fl}}\n",
    "\n",
    "        if self.save_state:\n",
    "            model_state = self.state_dict()\n",
    "            for each in model_state:\n",
    "                model_state[each] = model_state[each].tolist()\n",
    "            model_data['state_dict'] = model_state\n",
    "\n",
    "        json_save(model_data, file_name, direc)\n",
    "\n",
    "        # Scripting the model for compatibility with LibTorch\n",
    "        \n",
    "        self.reset_hidden(1)\n",
    "        \n",
    "        model_scripted = torch.jit.script(self)\n",
    "        # \n",
    "        # # Saving the scripted model\n",
    "        scripted_model_file = file_name + \"_scripted.pt\"\n",
    "        if direc:\n",
    "             scripted_model_file = os.path.join(direc, scripted_model_file)\n",
    "        model_scripted.save(scripted_model_file)\n",
    "        \n",
    "        # An example input you would normally provide to your model's forward() method.\n",
    "        example = torch.rand(1, 1, 1).to(torch.device(\"cpu\"))\n",
    "        onnx_model_file = file_name + \".onnx\"\n",
    "        if direc:\n",
    "             onnx_model_file = os.path.join(direc, onnx_model_file)\n",
    "        \n",
    "        torch.onnx.export(model=self,\n",
    "                          args=example,\n",
    "                          f=onnx_model_file,\n",
    "                          export_params=True,\n",
    "                          opset_version=13,\n",
    "                          do_constant_folding=True,\n",
    "                          input_names = ['input'],\n",
    "                          output_names = ['output'])\n",
    "\n",
    "    # train_epoch runs one epoch of training\n",
    "    def train_epoch(self, dataloader, loss_fcn, optim, bs, init_len=200, up_fr=1000):\n",
    "\n",
    "        # shuffle the segments at the start of the epoch\n",
    "        #shuffle = torch.randperm(input_data.shape[1])\n",
    "        \n",
    "        #print(shuffle)\n",
    "\n",
    "        self.reset_hidden(bs)\n",
    "        \n",
    "        # Iterate over the batches\n",
    "        ep_loss = 0\n",
    "        for batch_i, (X, y) in enumerate(dataloader):\n",
    "            # Load batch of shuffled segments\n",
    "            input_batch = X.to(torch.device(\"cpu\"))\n",
    "            target_batch = y.to(torch.device(\"cpu\"))\n",
    "\n",
    "\n",
    "            # Initialise network hidden state by processing some samples then zero the gradient buffers\n",
    "            # For training processing eine Anfangssequenz, damit ein brauchbarer hidden state vorliegt\n",
    "            # Training startet erst nach! einem eingelaufen hidden state\n",
    "            self(input_batch[0:init_len, :, :])\n",
    "            self.zero_grad()\n",
    "\n",
    "            # Choose the starting index for processing the rest of the batch sequence, in chunks of args.up_fr\n",
    "            start_i = init_len\n",
    "            batch_loss = 0\n",
    "            # Iterate over the remaining samples in the mini batch\n",
    "            for k in range(math.ceil((input_batch.shape[0] - init_len) / up_fr)):\n",
    "                # Process input batch with neural network\n",
    "                \n",
    "                print(input_batch[start_i:start_i + up_fr, :, :].shape)\n",
    "                \n",
    "                output = self(input_batch[start_i:start_i + up_fr, :, :])\n",
    "\n",
    "                # Calculate loss and update network parameters\n",
    "                loss = loss_fcn(output, target_batch[start_i:start_i + up_fr, :, :])\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Set the network hidden state, to detach it from the computation graph\n",
    "                # Die Gradienteninformation die einhergeht mit dem hidden state ist connected zu dem\n",
    "                # Computiaonal Graf von den vorherigen outputs. Da wir immer nur den letzten hidden state\n",
    "                # zur berechnung brauchen und den state auch nicht updaten wollen - wird er hier detached\n",
    "                # detached = wir löchen die Gradienteninformation \n",
    "                # https://discuss.pytorch.org/t/stupid-question-why-do-you-have-to-detach-the-hidden-state-of-lstms-but-not-the-hidden-state-of-a-linear-network/95089/3\n",
    "\n",
    "                \n",
    "                self.detach_hidden()\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Update the start index for the next iteration and add the loss to the batch_loss total\n",
    "                start_i += up_fr\n",
    "                batch_loss += loss\n",
    "\n",
    "            # Add the average batch loss to the epoch loss and reset the hidden states to zeros\n",
    "            ep_loss += batch_loss / (k + 1)\n",
    "        return ep_loss / (batch_i + 1)\n",
    "\n",
    "    # only proc processes a the input data and calculates the loss, optionally grad can be tracked or not\n",
    "    def process_data(self, dataloader, loss_fcn, chunk, grad=False, validate=False):\n",
    "        with (torch.no_grad() if not grad else nullcontext()):\n",
    "            self.reset_hidden(input_data.shape[1])\n",
    "            output = torch.empty_like(target_data)\n",
    "            for l in range(int(output.size()[0] / chunk)):\n",
    "                if validate:\n",
    "                    output[l * chunk:(l + 1) * chunk] = input_data[l * chunk:(l + 1) * chunk]\n",
    "                else:\n",
    "                    output[l * chunk:(l + 1) * chunk] = self(input_data[l * chunk:(l + 1) * chunk])\n",
    "                    self.detach_hidden()\n",
    "            # If the data set doesn't divide evenly into the chunk length, process the remainder\n",
    "            if not (output.size()[0] / chunk).is_integer():\n",
    "                if validate:\n",
    "                    output[(l + 1) * chunk:-1] = input_data[(l + 1) * chunk:-1]                    \n",
    "                else:\n",
    "                    output[(l + 1) * chunk:-1] = self(input_data[(l + 1) * chunk:-1])\n",
    "            loss = loss_fcn(output, target_data)\n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T16:33:32.641241700Z",
     "start_time": "2024-01-19T16:33:32.586721Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ESRLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.add(target, -output)\n",
    "        loss = torch.pow(loss, 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss\n",
    "class DCLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCLoss, self).__init__()\n",
    "        self.epsilon = 0.00001\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = torch.pow(torch.add(torch.mean(target, 0), -torch.mean(output, 0)), 2)\n",
    "        loss = torch.mean(loss)\n",
    "        energy = torch.mean(torch.pow(target, 2)) + self.epsilon\n",
    "        loss = torch.div(loss, energy)\n",
    "        return loss      \n",
    "class LossWrapper(nn.Module):\n",
    "    def __init__(self, losses):\n",
    "        super(LossWrapper, self).__init__()\n",
    "        loss_dict = {'ESR': ESRLoss(), 'DC': DCLoss()}\n",
    "\n",
    "        loss_functions = [[loss_dict[key], value] for key, value in losses.items()]\n",
    "\n",
    "        self.loss_functions = tuple([items[0] for items in loss_functions])\n",
    "        try:\n",
    "            self.loss_factors = tuple(torch.Tensor([items[1] for items in loss_functions]))\n",
    "        except IndexError:\n",
    "            self.loss_factors = torch.ones(len(self.loss_functions))\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = 0\n",
    "        for i, losses in enumerate(self.loss_functions):\n",
    "            loss += torch.mul(losses(output, target), self.loss_factors[i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "StatefulLSTM                             [1, 1, 1]                 --\n",
      "├─LSTM: 1-1                              [1, 1, 32]                4,480\n",
      "├─Linear: 1-2                            [1, 1, 1]                 33\n",
      "==========================================================================================\n",
      "Total params: 4,513\n",
      "Trainable params: 4,513\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "network = StatefulLSTM(input_size=1, \n",
    "                       output_size=1,\n",
    "                       hidden_size=32,\n",
    "                       skip=0)\n",
    "\n",
    "optimiser = torch.optim.Adam(network.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimiser, 'min', factor=0.5, patience=5, verbose=True)\n",
    "loss_functions = LossWrapper({\"ESR\": 0.75, \"DC\": 0.25})\n",
    "\n",
    "network.save_state = True\n",
    "\n",
    "summary = torchinfo.summary(network, (1, 1, 1), device=torch.device(\"cpu\"))\n",
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T17:22:30.128796800Z",
     "start_time": "2024-01-19T17:22:30.101357700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Define training procedure ############################################\n",
    "def train(dataloader, model, loss_functions, optimiser, batch_size, init_lenght, up_fr):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_loss = network.train_epoch(dataloader,\n",
    "                                     loss_functions,\n",
    "                                     optimiser,\n",
    "                                     batch_size,\n",
    "                                     init_lenght,\n",
    "                                     up_fr)\n",
    "    print(\"Epoch loss:\", epoch_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T17:22:36.338708300Z",
     "start_time": "2024-01-19T17:22:36.322649100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T17:19:47.194722300Z",
     "start_time": "2024-01-19T17:19:47.129432800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnetwork\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_functions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_lenght\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mup_fr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# Validation\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     val_output, val_loss \u001B[38;5;241m=\u001B[39m network\u001B[38;5;241m.\u001B[39mprocess_data(training_dataloader, loss_functions, chunk\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n",
      "Cell \u001B[1;32mIn[36], line 5\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(dataloader, model, loss_functions, optimiser, batch_size, init_lenght, up_fr)\u001B[0m\n\u001B[0;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m      4\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 5\u001B[0m epoch_loss \u001B[38;5;241m=\u001B[39m \u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mloss_functions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43moptimiser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43minit_lenght\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mup_fr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, epoch_loss)\n",
      "Cell \u001B[1;32mIn[19], line 84\u001B[0m, in \u001B[0;36mStatefulLSTM.train_epoch\u001B[1;34m(self, input_data, target_data, loss_fcn, optim, bs, init_len, up_fr)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_epoch\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_data, target_data, loss_fcn, optim, bs, init_len\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, up_fr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m):\n\u001B[0;32m     82\u001B[0m \n\u001B[0;32m     83\u001B[0m     \u001B[38;5;66;03m# shuffle the segments at the start of the epoch\u001B[39;00m\n\u001B[1;32m---> 84\u001B[0m     shuffle \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandperm(\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28mprint\u001B[39m(shuffle)\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_hidden(bs)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataLoader' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "best_val_los = 10000\n",
    "\n",
    "for t in range(1):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, network, loss_functions, optimiser, batch_size, init_lenght=200, up_fr=1000)\n",
    "    \n",
    "    # Validation\n",
    "    val_output, val_loss = network.process_data(training_dataloader, loss_functions, chunk=1000)\n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Val loss:\", val_loss)\n",
    "    \n",
    "    if val_loss < best_val_los:\n",
    "        network.reset_hidden(1)\n",
    "        network.save_model('model_best', save_path)\n",
    "    \n",
    "    #test(testing_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "### 0. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/\"+name+\"/\"+name+\".pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. On the test audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "X_testing shape:  torch.Size([1646977])\n",
      "X_ordered_testing shape:  torch.Size([1646828, 1, 150])\n",
      "y_testing shape:  (1646977,)\n",
      "prediction shape:  torch.Size([1646828])\n",
      "Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluate mode #################################\n",
    "model.eval()\n",
    "# Run prediction ##################################################\n",
    "prediction = torch.zeros(0).to(device)\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "with torch.no_grad():\n",
    "    for X, _ in testing_dataloader:\n",
    "        X = X.to(device)\n",
    "        predicted_batch = model(X)\n",
    "        prediction = torch.cat((prediction, predicted_batch.flatten()), 0)\n",
    "\n",
    "save_wav('models/'+name+'/y_pred.wav', prediction.cpu().numpy())\n",
    "save_wav('models/'+name+'/x_test.wav', X_testing.numpy())\n",
    "save_wav('models/'+name+'/y_test.wav', y_testing)\n",
    "\n",
    "print(\"X_testing shape: \", X_testing.shape)\n",
    "print(\"X_ordered_testing shape: \", X_ordered_testing.shape)\n",
    "print(\"y_testing shape: \", y_testing.shape)\n",
    "print(\"prediction shape: \", prediction.shape)\n",
    "\n",
    "print(\"Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. On a number sequence (to control inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_testing_2 shape: torch.Size([2, 1, 150])\n",
      "Running prediction..\n",
      "prediction tensor([[-0.0911],\n",
      "        [-0.2398]], grad_fn=<AddmmBackward0>)\n",
      "X_testing_2 shape:  torch.Size([2, 1, 150])\n",
      "prediction_2 shape:  torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size_test2 = 2\n",
    "\n",
    "# Test the model simple number sequence to compare with inference #\n",
    "X_testing_2 = np.array([], dtype=np.float64)\n",
    "\n",
    "for i in range(0, batch_size_test2 * input_size):\n",
    "    X_testing_2 = np.append(X_testing_2, i*0.001)\n",
    "\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "X_testing_2 = np.reshape(X_testing_2, (batch_size_test2, 1, input_size))\n",
    "\n",
    "X_testing_2 = torch.from_numpy(X_testing_2).double()\n",
    "\n",
    "print(f\"X_testing_2 shape: {X_testing_2.shape}\")\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "model = model.float()\n",
    "\n",
    "prediction_2 = model(X_testing_2.to(device).float())\n",
    "\n",
    "print(f\"prediction {prediction_2}\")\n",
    "\n",
    "print(\"X_testing_2 shape: \", X_testing_2.shape)\n",
    "print(\"prediction_2 shape: \", prediction_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as pt model\n",
    "### 1. for minimal examples (with batch size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_minimal = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_minimal, 1, input_size).to(device)\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "traced_script_module.save(\"models/\"+name+\"/\"+name+\"-minimal.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_minimal, 1, input_size).to(device)\n",
    "filepath = \"models/\"+name+\"/\"+name+\"-libtorch\"+\"-minimal.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model=model,\n",
    "                  args=example,\n",
    "                  f=filepath,\n",
    "                  export_params=True,\n",
    "                  opset_version=13,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. for real-time streaming (with batch size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_streaming = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_streaming, 1, input_size).to(device)\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "traced_script_module.save(\"models/\"+name+\"/\"+name+\"-streaming.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_streaming, 1, input_size).to(device)\n",
    "filepath = \"models/\"+name+\"/\"+name+\"-libtorch\"+\"-streaming.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model=model,\n",
    "                  args=example,\n",
    "                  f=filepath,\n",
    "                  export_params=True,\n",
    "                  opset_version=13,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
