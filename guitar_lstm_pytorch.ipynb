{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:37:52.844640500Z",
     "start_time": "2024-01-19T15:37:49.370233500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchinfo\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose computation device (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:37:56.045079300Z",
     "start_time": "2024-01-19T15:37:56.008184800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device not found.\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (\"MPS device found.\")\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "    \n",
    "# Select the device for training\n",
    "device = cpu_device\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:38:20.944539500Z",
     "start_time": "2024-01-19T15:38:20.932209900Z"
    }
   },
   "outputs": [],
   "source": [
    "# EDIT THIS SECTION FOR USER INPUTS\n",
    "#\n",
    "name = 'model_1'\n",
    "in_file = 'TrainingData/flanger-input.wav'\n",
    "out_file = 'TrainingData/flanger-target.wav'\n",
    "epochs = 1\n",
    "\n",
    "input_size = 1 \n",
    "batch_size = 4096 \n",
    "test_size = 0.2\n",
    "\n",
    "if not os.path.exists('models/'+name):\n",
    "    os.makedirs('models/'+name)\n",
    "else:\n",
    "    print(\"A model with the same name already exists. Please choose a new name.\")\n",
    "    exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-19T15:38:23.398499800Z",
     "start_time": "2024-01-19T15:38:23.369242600Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_wav(name, data):\n",
    "    sp.io.wavfile.write(name, 44100, data.flatten().astype(np.float32))\n",
    "\n",
    "def normalize(data):\n",
    "    data_max = max(data)\n",
    "    data_min = min(data)\n",
    "    data_norm = max(data_max,abs(data_min))\n",
    "    return data / data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T13:34:52.971175200Z",
     "start_time": "2024-01-17T13:34:50.700465500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training shape (pre-processing): (352800,)\n",
      "y_training shape (pre-processing): (352800,)\n",
      "X_testing shape (pre-processing): (88200,)\n",
      "y_testing shape (pre-processing): (88200,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "gather(): Expected dtype int64 for index",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 25\u001B[0m\n\u001B[0;32m     23\u001B[0m X_ordered_training \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(indices, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(indices):\n\u001B[1;32m---> 25\u001B[0m     X_ordered_training[i] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_training\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m X_ordered_training \u001B[38;5;241m=\u001B[39m X_ordered_training\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX_ordered_training shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX_ordered_training\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: gather(): Expected dtype int64 for index"
     ]
    }
   ],
   "source": [
    "# Load and Preprocess Data ###########################################\n",
    "in_rate, in_data = sp.io.wavfile.read(in_file)\n",
    "out_rate, out_data = sp.io.wavfile.read(out_file)\n",
    "\n",
    "X_all = in_data.astype(np.float32).flatten()  \n",
    "X_all = normalize(X_all)\n",
    "y_all = out_data.astype(np.float32).flatten() \n",
    "y_all = normalize(y_all)\n",
    "\n",
    "# Get the last 20% of the wav data for testing and thee rest for training\n",
    "X_training, X_testing = np.split(X_all, [int(len(X_all)*(1-test_size))])\n",
    "y_training, y_testing = np.split(y_all, [int(len(y_all)*(1-test_size))])\n",
    "print(f\"X_training shape (pre-processing): {X_training.shape}\")\n",
    "print(f\"y_training shape (pre-processing): {y_training.shape}\")\n",
    "print(f\"X_testing shape (pre-processing): {X_testing.shape}\")\n",
    "print(f\"y_testing shape (pre-processing): {y_testing.shape}\")\n",
    "\n",
    "# Create a new array where each element is an array of input_size samples in time order\n",
    "# Each element of the new array is shifted by one sample from the previous element\n",
    "indices = np.arange(input_size) + np.arange(len(X_training)-input_size+1)[:,np.newaxis]\n",
    "indices = torch.from_numpy(indices)\n",
    "X_training = torch.from_numpy(X_training)\n",
    "X_ordered_training = torch.zeros_like(indices, dtype=torch.float32)\n",
    "for i, j in enumerate(indices):\n",
    "    X_ordered_training[i] = torch.gather(X_training, 0, indices[i])\n",
    "X_ordered_training = X_ordered_training.unsqueeze(1)\n",
    "print(f\"X_ordered_training shape: {X_ordered_training.shape}\")\n",
    "\n",
    "indices = np.arange(input_size) + np.arange(len(X_testing)-input_size+1)[:,np.newaxis]\n",
    "indices = torch.from_numpy(indices)\n",
    "X_testing = torch.from_numpy(X_testing)\n",
    "X_ordered_testing = torch.zeros_like(indices, dtype=torch.float32)\n",
    "for i, j in enumerate(indices):\n",
    "    X_ordered_testing[i] = torch.gather(X_testing, 0, indices[i])\n",
    "X_ordered_testing = X_ordered_testing.unsqueeze(1)\n",
    "print(f\"X_ordered_testing shape: {X_ordered_testing.shape}\")\n",
    "\n",
    "\n",
    "# The input size defines the number of samples used for each prediction\n",
    "# Therefore the first output value that we get is at index input_size-1\n",
    "y_ordered_training = y_training[input_size-1:]\n",
    "y_ordered_training = torch.from_numpy(y_ordered_training)\n",
    "y_ordered_training = y_ordered_training.unsqueeze(1)\n",
    "print(f\"y_ordered_training shape: {y_ordered_training.shape}\")\n",
    "\n",
    "y_ordered_testing = y_testing[input_size-1:]\n",
    "y_ordered_testing = torch.from_numpy(y_ordered_testing)\n",
    "y_ordered_testing = y_ordered_testing.unsqueeze(1)\n",
    "print(f\"y_ordered_testing shape: {y_ordered_testing.shape}\")\n",
    "\n",
    "print(f\"The X_ordered_training data is an array, where each element is an array of input_size samples in time order. Therefore the lenght is smaller than the original X_training array (the first {input_size} samples are grouped).\")\n",
    "print(f\"The y_ordered_training data is an array, where each element is a single sample. This single sample is the target output for the corresponding X_random_training element, which consists of input samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Shape of X: torch.Size([4096, 1, 150])\n",
      "Shape of y: torch.Size([4096, 1]) torch.float32\n",
      "Batch: 0\n",
      "Shape of X: torch.Size([4096, 1, 150])\n",
      "Shape of y: torch.Size([4096, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "training_dataset = torch.utils.data.TensorDataset(X_ordered_training, y_ordered_training)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch, (X, y) in enumerate(training_dataloader):\n",
    "    print(f\"Batch: {batch}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "testing_dataset = torch.utils.data.TensorDataset(X_ordered_testing, y_ordered_testing)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for batch, (X, y) in enumerate(testing_dataloader):\n",
    "    print(f\"Batch: {batch}\")\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "NeuralNetwork                            [1, 1]                    --\n",
      "├─ConstantPad1d: 1-1                     [1, 1, 174]               --\n",
      "├─Conv1d: 1-2                            [1, 16, 14]               208\n",
      "├─ConstantPad1d: 1-3                     [1, 16, 38]               --\n",
      "├─Conv1d: 1-4                            [1, 16, 3]                3,088\n",
      "├─LSTM: 1-5                              [1, 3, 36]                7,776\n",
      "├─Linear: 1-6                            [1, 1]                    37\n",
      "==========================================================================================\n",
      "Total params: 11,109\n",
      "Trainable params: 11,109\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.04\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.05\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "'''This is a similar PyTorch implementation of the LSTM model from the paper:\n",
    "    \"Real-Time Guitar Amplifier Emulation with Deep Learning\"\n",
    "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
    "\n",
    "    Uses a stack of two 1-D Convolutional layers, followed by LSTM, followed by \n",
    "    a Dense (fully connected) layer. Three preset training modes are available, \n",
    "    with further customization by editing the code. A PyTorch model \n",
    "    is implemented here.\n",
    "\n",
    "    Note: RAM may be a limiting factor for the parameter \"input_size\". The wav data\n",
    "      is preprocessed and stored in RAM, which improves training speed but quickly runs out\n",
    "      if using a large number for \"input_size\".  Reduce this if you are experiencing\n",
    "      RAM issues.\n",
    "'''\n",
    "\n",
    "if train_mode == 0:         # Speed Training\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 12   \n",
    "    conv1d_1_strides = 12\n",
    "    conv1d_filters = 16\n",
    "    hidden_units = 36\n",
    "elif train_mode == 1:       # Accuracy Training (~10x longer than Speed Training)\n",
    "    learning_rate = 0.01 \n",
    "    conv1d_strides = 4\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 64\n",
    "else:                       # Extended Training (~60x longer than Accuracy Training)\n",
    "    learning_rate = 0.0005 \n",
    "    conv1d_strides = 3\n",
    "    conv1d_filters = 36\n",
    "    hidden_units= 96\n",
    "\n",
    "# Define model ########################################################\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ConstantPad1d(padding=12, value=0)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=conv1d_filters, kernel_size=12, stride=conv1d_strides)\n",
    "        self.conv2 = nn.Conv1d(in_channels=conv1d_filters, out_channels=conv1d_filters, kernel_size=12, stride=conv1d_strides)\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=hidden_units, batch_first = True, bias=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_units, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pad(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        output, (hidden, cell) =  self.lstm(x)\n",
    "        x = self.linear(output[:, -1, :])\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "summary = torchinfo.summary(model, (1, 1, 150), device=device)\n",
    "print(summary)\n",
    "\n",
    "# Define loss function and optimizer ##################################\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training procedure ############################################\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Preprocess input and target data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Define training procedure ############################################\n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.036848  [ 4096/6587758]\n",
      "loss: 0.002071  [413696/6587758]\n",
      "loss: 0.000916  [823296/6587758]\n",
      "loss: 0.000708  [1232896/6587758]\n",
      "loss: 0.000610  [1642496/6587758]\n",
      "loss: 0.000456  [2052096/6587758]\n",
      "loss: 0.000445  [2461696/6587758]\n",
      "loss: 0.000411  [2871296/6587758]\n",
      "loss: 0.000395  [3280896/6587758]\n",
      "loss: 0.000417  [3690496/6587758]\n",
      "loss: 0.000351  [4100096/6587758]\n",
      "loss: 0.000365  [4509696/6587758]\n",
      "loss: 0.000347  [4919296/6587758]\n",
      "loss: 0.000342  [5328896/6587758]\n",
      "loss: 0.000322  [5738496/6587758]\n",
      "loss: 0.000341  [6148096/6587758]\n",
      "loss: 0.000338  [6557696/6587758]\n",
      "Test Error: \n",
      " Avg loss: 0.000289 \n",
      "\n",
      "Done!\n",
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_dataloader, model, loss_fn, optimizer)\n",
    "    test(testing_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"models/\"+name+\"/\"+name+\".pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions\n",
    "### 0. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/\"+name+\"/\"+name+\".pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. On the test audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction..\n",
      "X_testing shape:  torch.Size([1646977])\n",
      "X_ordered_testing shape:  torch.Size([1646828, 1, 150])\n",
      "y_testing shape:  (1646977,)\n",
      "prediction shape:  torch.Size([1646828])\n",
      "Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluate mode #################################\n",
    "model.eval()\n",
    "# Run prediction ##################################################\n",
    "prediction = torch.zeros(0).to(device)\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "with torch.no_grad():\n",
    "    for X, _ in testing_dataloader:\n",
    "        X = X.to(device)\n",
    "        predicted_batch = model(X)\n",
    "        prediction = torch.cat((prediction, predicted_batch.flatten()), 0)\n",
    "\n",
    "save_wav('models/'+name+'/y_pred.wav', prediction.cpu().numpy())\n",
    "save_wav('models/'+name+'/x_test.wav', X_testing.numpy())\n",
    "save_wav('models/'+name+'/y_test.wav', y_testing)\n",
    "\n",
    "print(\"X_testing shape: \", X_testing.shape)\n",
    "print(\"X_ordered_testing shape: \", X_ordered_testing.shape)\n",
    "print(\"y_testing shape: \", y_testing.shape)\n",
    "print(\"prediction shape: \", prediction.shape)\n",
    "\n",
    "print(\"Note that the prediction shape is smaller than the y_testing shape. This is because the first predicted sample needs input_size samples for prediction.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. On a number sequence (to control inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_testing_2 shape: torch.Size([2, 1, 150])\n",
      "Running prediction..\n",
      "prediction tensor([[-0.0911],\n",
      "        [-0.2398]], grad_fn=<AddmmBackward0>)\n",
      "X_testing_2 shape:  torch.Size([2, 1, 150])\n",
      "prediction_2 shape:  torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "batch_size_test2 = 2\n",
    "\n",
    "# Test the model simple number sequence to compare with inference #\n",
    "X_testing_2 = np.array([], dtype=np.float64)\n",
    "\n",
    "for i in range(0, batch_size_test2 * input_size):\n",
    "    X_testing_2 = np.append(X_testing_2, i*0.001)\n",
    "\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "X_testing_2 = np.expand_dims(X_testing_2, axis=0)\n",
    "X_testing_2 = np.reshape(X_testing_2, (batch_size_test2, 1, input_size))\n",
    "\n",
    "X_testing_2 = torch.from_numpy(X_testing_2).double()\n",
    "\n",
    "print(f\"X_testing_2 shape: {X_testing_2.shape}\")\n",
    "\n",
    "print(\"Running prediction..\")\n",
    "model = model.float()\n",
    "\n",
    "prediction_2 = model(X_testing_2.to(device).float())\n",
    "\n",
    "print(f\"prediction {prediction_2}\")\n",
    "\n",
    "print(\"X_testing_2 shape: \", X_testing_2.shape)\n",
    "print(\"prediction_2 shape: \", prediction_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as pt model\n",
    "### 1. for minimal examples (with batch size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_minimal = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_minimal, 1, input_size).to(device)\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "traced_script_module.save(\"models/\"+name+\"/\"+name+\"-minimal.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_minimal, 1, input_size).to(device)\n",
    "filepath = \"models/\"+name+\"/\"+name+\"-libtorch\"+\"-minimal.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model=model,\n",
    "                  args=example,\n",
    "                  f=filepath,\n",
    "                  export_params=True,\n",
    "                  opset_version=13,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. for real-time streaming (with batch size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_streaming = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_streaming, 1, input_size).to(device)\n",
    "\n",
    "# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "traced_script_module = torch.jit.trace(model, example)\n",
    "traced_script_module.save(\"models/\"+name+\"/\"+name+\"-streaming.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/_internal/jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/Users/faresschulz/Desktop/ADC-talk/GuitarLSTM/venv/lib/python3.11/site-packages/torch/onnx/utils.py:1209: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# An example input you would normally provide to your model's forward() method.\n",
    "example = torch.rand(batch_size_streaming, 1, input_size).to(device)\n",
    "filepath = \"models/\"+name+\"/\"+name+\"-libtorch\"+\"-streaming.onnx\"\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model=model,\n",
    "                  args=example,\n",
    "                  f=filepath,\n",
    "                  export_params=True,\n",
    "                  opset_version=13,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
